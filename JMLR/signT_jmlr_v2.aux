\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{anandkumar2014tensor}
\citation{zhang2019tensor}
\citation{hore2016tensor}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Motivation}{1}{subsection.1.1}}
\newlabel{sec:motivation}{{1.1}{1}{Motivation}{subsection.1.1}{}}
\newlabel{eq:modelintro}{{1}{1}{Motivation}{equation.1.1}{}}
\citation{jain2014provable,montanari2018spectral,anandkumar2014tensor,allen2012sparse}
\citation{hitchcock1927expression}
\citation{de2000multilinear}
\citation{wang2019multiway}
\citation{wang2018learning,hu2021generalized}
\citation{ghadermarzy2018learning}
\citation{hore2016tensor}
\citation{chan2014consistent}
\MT@newlabel{eq:modelintro}
\citation{anandkumar2014tensor,montanari2018spectral,cai2019nonconvex}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Numerical tensor rank (CP and Tucker) vs.\ transformation level $c$ in the first example. For Tucker rank, we plot the rank along first mode for illustration. (b) Top $d=30$ tensor singular values in the second example. The details of experiment are provided in Appendix\nobreakspace  {}\ref  {sec:numericalrank}.\relax }}{3}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{3}{(a) Numerical tensor rank (CP and Tucker) vs.\ transformation level $c$ in the first example. For Tucker rank, we plot the rank along first mode for illustration. (b) Top $d=30$ tensor singular values in the second example. The details of experiment are provided in Appendix~\ref {sec:numericalrank}.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Our contributions}{3}{subsection.1.2}}
\citation{wang2018learning,han2020optimal,ghadermarzy2018learning}
\citation{chatterjee2015matrix,zhang2017estimating}
\citation{ganti2015matrix}
\citation{pmlr-v70-ongie17a,fan2019online}
\citation{lee2021nonparametric}
\citation{anandkumar2017analyzing}
\citation{kolda2003counterexample}
\citation{chatterjee2015matrix,xu2018rates}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Related work}{4}{subsection.1.3}}
\citation{hao2019sparse}
\citation{zhou2020broadcasted}
\citation{klopp2017oracle,gao2015rate,chan2014consistent}
\citation{zhao2015hypergraph,lovasz2012large}
\citation{zhao2015hypergraph}
\citation{balasubramanian2021nonparametric}
\citation{balasubramanian2021nonparametric}
\citation{hitchcock1927expression}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Notation and organization}{5}{subsection.1.4}}
\newlabel{eq:CP}{{2}{6}{Notation and organization}{equation.1.2}{}}
\MT@newlabel{eq:CP}
\@writefile{toc}{\contentsline {section}{\numberline {2}Sign representable tensor model}{6}{section.2}}
\newlabel{sec:overview}{{2}{6}{Notation and organization}{section.2}{}}
\newlabel{eq:model}{{3}{6}{Notation and organization}{equation.2.3}{}}
\MT@newlabel{eq:model}
\citation{cohn2013fast}
\citation{alon2016sign}
\citation{de2003nondeterministic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Signal tensor model}{7}{subsection.2.1}}
\MT@newlabel{eq:model}
\newlabel{eq:sign-rank}{{4}{7}{Sign-rank}{equation.2.4}{}}
\MT@newlabel{eq:model}
\newlabel{def:sign}{{5}{7}{Sign representable tensor model}{equation.2.5}{}}
\MT@newlabel{def:sign}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Noise tensor model}{7}{subsection.2.2}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\citation{kolda2009tensor}
\citation{wang2019multiway,chi2020provable}
\citation{wang2018learning}
\citation{hong2020generalized}
\citation{robinson1988root}
\citation{balabdaoui2019least,ganti2017learning}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Important examples}{8}{subsection.2.3}}
\newlabel{sec:common}{{2.3}{8}{Important examples}{subsection.2.3}{}}
\newlabel{eq:example}{{5}{8}{Structured tensors with repeating entries}{example.5}{}}
\citation{hillar2013most}
\citation{alon2016sign}
\@writefile{toc}{\contentsline {section}{\numberline {3}Algebraic properties of sign representable tensors}{9}{section.3}}
\newlabel{sec:algebry}{{3}{9}{Important examples}{section.3}{}}
\MT@newlabel{def:sign}
\newlabel{cor:monotonic}{{1}{9}{Relationship between sign-rank and usual rank}{prop.1}{}}
\newlabel{example:max}{{6}{9}{Structured tensors with repeating entries}{example.6}{}}
\newlabel{prop:repeat}{{2}{9}{Rank relations for structured tensors with repeating entries}{prop.2}{}}
\newlabel{eq:max}{{6}{9}{Rank relations for structured tensors with repeating entries}{equation.3.6}{}}
\MT@newlabel{eq:max}
\newlabel{example:banded}{{7}{10}{Stacked banded matrices}{example.7}{}}
\newlabel{prop:connection}{{3}{10}{Rank relations for stacked banded tensors}{prop.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Statistical properties of sign representable tensors}{10}{section.4}}
\newlabel{sec:stat}{{4}{10}{Important examples}{section.4}{}}
\MT@newlabel{eq:model}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Classification risk with $l_1$ weights}{10}{subsection.4.1}}
\newlabel{sec:proposal}{{4.1}{10}{Classification risk with $l_1$ weights}{subsection.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Main notation used in Sections\nobreakspace  {}\ref  {sec:stat}-\ref  {sec:estimation}.\relax }}{11}{table.caption.2}}
\newlabel{tb:not}{{1}{11}{Main notation used in Sections~\ref {sec:stat}-\ref {sec:estimation}.\relax }{table.caption.2}{}}
\newlabel{eq:sample}{{7}{11}{Classification risk with $l_1$ weights}{equation.4.7}{}}
\MT@newlabel{eq:sample}
\newlabel{eq:population}{{8}{11}{Classification risk with $l_1$ weights}{equation.4.8}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:population}
\newlabel{prop:global}{{1}{11}{Weighted classification risk minimizer}{thm.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:population}
\newlabel{eq:optimal}{{9}{11}{Weighted classification risk minimizer}{equation.4.9}{}}
\MT@newlabel{eq:optimal}
\newlabel{rmk:landscape}{{3}{12}{Landscape of risk minimizers}{rmk.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Uniquness of risk minimizer}{12}{subsection.4.2}}
\newlabel{sec:identifiability}{{4.2}{12}{Uniquness of risk minimizer}{subsection.4.2}{}}
\newlabel{eq:cdf}{{4.2}{12}{Uniquness of risk minimizer}{subsection.4.2}{}}
\newlabel{ass:margin}{{3}{12}{$\alpha $-smoothness of discrete distribution}{defn.3}{}}
\newlabel{eq:smooth}{{10}{12}{$\alpha $-smoothness of discrete distribution}{equation.4.10}{}}
\citation{xu2018rates,balasubramanian2021nonparametric}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Three examples of CDF with smoothness index $\alpha $ at $\pi $ depicted in dashed lines. Figure (a)-(b): Function $G(\pi )$ has $\alpha = 1$ with $\mathcal  {N}= \emptyset $, because the $G(\pi )$ has finite pseudo density in the range of $\pi $. Figure (c): Function $G(\pi )$ has $\alpha = \infty $ at most $\pi $'s (in blue), except for a total of $|\mathcal  {N}|_{\text  {jump}}=r$ non-negligible jump points (in red).\relax }}{13}{figure.caption.3}}
\newlabel{fig:cdf}{{2}{13}{Three examples of CDF with smoothness index $\alpha $ at $\pi $ depicted in dashed lines. Figure (a)-(b): Function $G(\pi )$ has $\alpha = 1$ with $\tN = \emptyset $, because the $G(\pi )$ has finite pseudo density in the range of $\pi $. Figure (c): Function $G(\pi )$ has $\alpha = \infty $ at most $\pi $'s (in blue), except for a total of $|\tN |_{\text {jump}}=r$ non-negligible jump points (in red).\relax }{figure.caption.3}{}}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:optimal}
\newlabel{thm:population}{{2}{13}{Perturbation bound of risk minimizer}{thm.2}{}}
\newlabel{eq:identity}{{11}{13}{Perturbation bound of risk minimizer}{equation.4.11}{}}
\MT@newlabel{eq:identity}
\newlabel{rmk:graphon}{{6}{14}{Comparison with hypergraphon models}{rmk.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Why not $l_q$ weight for $q\not =1$?}{14}{subsection.4.3}}
\newlabel{sec:fail}{{4.3}{14}{Why not $l_q$ weight for $q\neq 1$?}{subsection.4.3}{}}
\MT@newlabel{eq:sample}
\newlabel{eq:gsample}{{4.3}{14}{Why not $l_q$ weight for $q\neq 1$?}{subsection.4.3}{}}
\newlabel{prop:global2}{{3}{14}{Generalized weighted classification risk minimizer}{thm.3}{}}
\newlabel{eq:optimal_detail}{{12}{14}{Generalized weighted classification risk minimizer}{equation.4.12}{}}
\newlabel{eq:optimal_q}{{13}{14}{Generalized weighted classification risk minimizer}{equation.4.13}{}}
\MT@newlabel{eq:optimal_detail}
\MT@newlabel{eq:optimal_q}
\MT@newlabel{eq:sample}
\@writefile{toc}{\contentsline {section}{\numberline {5}Finite sample accuracy for signal tensor estimation}{15}{section.5}}
\newlabel{sec:estimation}{{5}{15}{Why not $l_q$ weight for $q\neq 1$?}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Learning-reduction framework to signal estimation}{15}{subsection.5.1}}
\newlabel{eq:est}{{14}{15}{Learning-reduction framework to signal estimation}{equation.5.14}{}}
\MT@newlabel{eq:sample}
\citation{ghadermarzy2018learning,wang2018learning,hong2020generalized}
\citation{ghadermarzy2018learning,wang2018learning,hong2020generalized,han2020optimal}
\citation{wang2018learning}
\citation{han2020optimal}
\citation{wang2018learning,hong2020generalized}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of our method in the context of an order-2 tensor (i.e. matrix). (a): a noisy, incomplete tensor input. (b)-(c): Estimation of sign tensor series $\textup  {sgn}(\Theta -\pi )$ for $\pi \in \{-1,\ldots  ,-{1/ H},0,{1/H},\ldots  ,1\}$. (d): recovered signal $\mathaccentV {hat}05E\Theta $. The depicted signal is a full-rank matrix based on Example\nobreakspace  {}\ref  {eq:example} in Section\nobreakspace  {}\ref  {sec:common}.\relax }}{16}{figure.caption.4}}
\newlabel{fig:demo}{{3}{16}{Illustration of our method in the context of an order-2 tensor (i.e. matrix). (a): a noisy, incomplete tensor input. (b)-(c): Estimation of sign tensor series $\sign (\Theta -\pi )$ for $\pi \in \{-1,\ldots ,-{1/ H},0,{1/H},\ldots ,1\}$. (d): recovered signal $\hat \Theta $. The depicted signal is a full-rank matrix based on Example~\ref {eq:example} in Section~\ref {sec:common}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Implementation details}{16}{subsection.5.2}}
\MT@newlabel{eq:est}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Nonparametric tensor completion via learning-reduction\relax }}{16}{algorithm.1}}
\newlabel{alg:tensorT}{{1}{16}{Nonparametric tensor completion via learning-reduction\relax }{algorithm.1}{}}
\newlabel{eq:base}{{15}{16}{Nonparametric tensor completion via learning-reduction\relax }{equation.5.15}{}}
\newlabel{eq:signal}{{16}{16}{Nonparametric tensor completion via learning-reduction\relax }{equation.5.16}{}}
\MT@newlabel{eq:base}
\MT@newlabel{eq:CP}
\MT@newlabel{eq:base}
\citation{wang2018learning,han2020optimal}
\citation{genzel2020robust,he2017kernelized}
\MT@newlabel{eq:base}
\MT@newlabel{eq:base}
\MT@newlabel{eq:est}
\MT@newlabel{eq:signal}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Finite sample accuracy}{17}{subsection.5.3}}
\newlabel{sec:error}{{5.3}{17}{Finite sample accuracy}{subsection.5.3}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:sample}
\newlabel{thm:estimation}{{4}{17}{Estimation error for sign representable tensors}{thm.4}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:base}
\MT@newlabel{eq:signal}
\citation{tsybakov2004optimal,xu2020class}
\newlabel{eq:bound}{{17}{18}{Estimation error for sign representable tensors}{equation.5.17}{}}
\newlabel{eq:bound2}{{18}{18}{Estimation error for sign representable tensors}{equation.5.18}{}}
\MT@newlabel{eq:bound2}
\newlabel{eq:special}{{19}{18}{Finite sample accuracy}{equation.5.19}{}}
\MT@newlabel{eq:bound}
\MT@newlabel{eq:bound}
\MT@newlabel{eq:bound}
\MT@newlabel{eq:bound}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:bound}
\MT@newlabel{eq:special}
\MT@newlabel{eq:bound}
\citation{wang2018learning,ghadermarzy2018learning,zhang2018tensor}
\citation{yuan2016tensor,ghadermarzy2019near,pmlr-v119-lee20i}
\citation{wang2019multiway}
\MT@newlabel{eq:bound2}
\newlabel{cor:completion}{{1}{19}{Sample complexity for nonparametric completion}{cor.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Revisiting earlier examples}{19}{subsection.5.4}}
\citation{zhang2018tensor,wang2018learning}
\citation{ganti2015matrix}
\citation{xu2018rates}
\citation{wang2019multiway}
\citation{ganti2015matrix}
\citation{wang2018learning}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of our statistical rates compared to existing works under different models. For notational simplicity, we present error rates assuming equal tensor dimension in all modes and finite $|\mathcal  {N}|_{\text  {jump}}$ for the smooth tensor model. Here $K\geq 2$ denotes the tensor order and $d$ denotes the tensor dimension.\relax }}{20}{table.caption.5}}
\newlabel{tb:comparison}{{2}{20}{Summary of our statistical rates compared to existing works under different models. For notational simplicity, we present error rates assuming equal tensor dimension in all modes and finite $|\tN |_{\text {jump}}$ for the smooth tensor model. Here $K\geq 2$ denotes the tensor order and $d$ denotes the tensor dimension.\relax }{table.caption.5}{}}
\citation{anandkumar2014tensor}
\citation{allen2012sparse}
\citation{wang2019multiway}
\citation{adomavicius2011context}
\citation{nickel2011three}
\citation{wang2019common}
\@writefile{toc}{\contentsline {section}{\numberline {6}Extensions}{21}{section.6}}
\newlabel{sec:extension}{{6}{21}{Revisiting earlier examples}{section.6}{}}
\MT@newlabel{eq:model}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sub-Gaussian tensor denoising}{21}{subsection.6.1}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{assm:subg}{{1}{21}{Sub-Gaussian noise}{assumption.1}{}}
\MT@newlabel{eq:sample}
\newlabel{thm:unbddno1}{{2}{21}{sub-Gaussian tensor denoising with nonparametric signals}{cor.2}{}}
\newlabel{eq:bound3}{{2}{21}{sub-Gaussian tensor denoising with nonparametric signals}{cor.2}{}}
\citation{jdanov2019human}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Probabilistic tensor estimation}{22}{subsection.6.2}}
\newlabel{eq:bernoulli}{{20}{22}{Probabilistic tensor estimation}{equation.6.20}{}}
\MT@newlabel{eq:bernoulli}
\MT@newlabel{eq:model}
\MT@newlabel{eq:sample}
\newlabel{thm:bernoulli}{{3}{22}{Probabilistic tensor estimation with nonparametric signals}{cor.3}{}}
\MT@newlabel{eq:bernoulli}
\newlabel{eq:bound4}{{3}{22}{Probabilistic tensor estimation with nonparametric signals}{cor.3}{}}
\MT@newlabel{eq:signal}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Binomial tensor problem}{22}{subsection.6.3}}
\newlabel{eq:binom}{{21}{22}{Binomial tensor problem}{equation.6.21}{}}
\citation{bartlett2006convexity,genzel2020robust,he2017kernelized}
\citation{scott2011surrogate}
\newlabel{thm:binom}{{4}{23}{Binomial tensor problem with nonparametric signals}{cor.4}{}}
\MT@newlabel{eq:binom}
\newlabel{eq:bound5}{{4}{23}{Binomial tensor problem with nonparametric signals}{cor.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Extension to hinge loss}{23}{subsection.6.4}}
\newlabel{sec:implementation}{{6.4}{23}{Extension to hinge loss}{subsection.6.4}{}}
\MT@newlabel{eq:base}
\MT@newlabel{eq:signal}
\newlabel{eq:hingeest}{{22}{23}{Extension to hinge loss}{equation.6.22}{}}
\MT@newlabel{eq:population}
\newlabel{eq:fisher}{{23}{23}{Extension to hinge loss}{equation.6.23}{}}
\citation{wang2018learning}
\citation{shen2003psi}
\citation{bartlett2006convexity}
\newlabel{ass:loss}{{2}{24}{Approximation bias}{assumption.2}{}}
\MT@newlabel{eq:hingeest}
\newlabel{thm:extension}{{5}{24}{Hinge loss based estimation}{thm.5}{}}
\newlabel{eq:rfs}{{24}{24}{Hinge loss based estimation}{equation.6.24}{}}
\MT@newlabel{eq:fisher}
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical experiment}{24}{section.7}}
\newlabel{sec:simulation}{{7}{24}{Extension to hinge loss}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Synthetic data}{24}{subsection.7.1}}
\citation{wang2017bayesian}
\citation{globerson2007euclidean}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Simulation models used for comparison. We use $\bm  {M}_k\in \{0,1\}^{d\times 3}$ to denote membership matrices, $\mathcal  {C}\in \mathbb  {R}^{3\times 3\times 3}$ the block means, $\bm  {a}=d^{-1}(1,2,\ldots  ,d)^T \in \mathbb  {R}^d$, $\mathcal  {Z}_{\qopname  \relax m{max}}$ and $\mathcal  {Z}_{\qopname  \relax m{min}}$ are order-3 tensors with entries $\qopname  \relax m{max}(i,j,k)/d$ and $\qopname  \relax m{min}(i,j,k)/d$, respectively.\relax }}{25}{table.caption.6}}
\newlabel{tab:simulation}{{3}{25}{Simulation models used for comparison. We use $\mM _k\in \{0,1\}^{d\times 3}$ to denote membership matrices, $\tC \in \mathbb {R}^{3\times 3\times 3}$ the block means, $\ma =d^{-1}(1,2,\ldots ,d)^T \in \mathbb {R}^d$, $\tZ _{\max }$ and $\tZ _{\min }$ are order-3 tensors with entries $\max (i,j,k)/d$ and $\min (i,j,k)/d$, respectively.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Estimation error versus tensor dimension. Panels (a)-(d) correspond to simulation models 1-4 in Table\nobreakspace  {}\ref  {tab:simulation}.\relax }}{25}{figure.caption.7}}
\newlabel{fig:compare1}{{4}{25}{Estimation error versus tensor dimension. Panels (a)-(d) correspond to simulation models 1-4 in Table~\ref {tab:simulation}.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Brain connectivity analysis}{25}{subsection.7.2}}
\citation{li2009brain,wang2017bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Completion error versus observation fraction. Panels (a)-(d) correspond to simulation models 1-4 in Table\nobreakspace  {}\ref  {tab:simulation}. \relax }}{26}{figure.caption.8}}
\newlabel{fig:compare2}{{5}{26}{Completion error versus observation fraction. Panels (a)-(d) correspond to simulation models 1-4 in Table~\ref {tab:simulation}. \relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Estimation error versus rank under different missing rate. Panels (a)-(d) correspond to missing rate 20\%, 33\%, 50\%, and 67\%, respectively. Error bar represents the standard error over 5-fold cross-validations.\relax }}{26}{figure.caption.9}}
\newlabel{fig:braincv}{{6}{26}{Estimation error versus rank under different missing rate. Panels (a)-(d) correspond to missing rate 20\%, 33\%, 50\%, and 67\%, respectively. Error bar represents the standard error over 5-fold cross-validations.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces MAE comparison in the brain data and NIPS data analysis. Reported MAEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Bold numbers indicate the minimal MAE among three methods. For low-rank CPT, we use R function {\tt  rTensor} with default hyperparameters, and for our method, we set $H=20$.\relax }}{27}{table.caption.10}}
\newlabel{tab:data}{{4}{27}{MAE comparison in the brain data and NIPS data analysis. Reported MAEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Bold numbers indicate the minimal MAE among three methods. For low-rank CPT, we use R function {\tt rTensor} with default hyperparameters, and for our method, we set $H=20$.\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}NIPS data analysis}{27}{subsection.7.3}}
\citation{xu2018rates}
\citation{xu2018rates,zhang2018network}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Estimated signal tensors in the data analysis. (a) top edges associated with IQ scores in the brain connectivity data. The color indicates the estimated IQ effect size. (b) top authors and words for years 1996-1999 in the NIPS data. Authors and words are ranked by marginal averages based on $\mathaccentV {hat}05E\Theta $, where the marginal average is denoted in the parentheses. \relax }}{28}{figure.caption.11}}
\newlabel{fig:signal}{{7}{28}{Estimated signal tensors in the data analysis. (a) top edges associated with IQ scores in the brain connectivity data. The color indicates the estimated IQ effect size. (b) top authors and words for years 1996-1999 in the NIPS data. Authors and words are ranked by marginal averages based on $\hat \Theta $, where the marginal average is denoted in the parentheses. \relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{28}{section.8}}
\newlabel{sec:conclusion}{{8}{28}{NIPS data analysis}{section.8}{}}
\citation{balasubramanian2021nonparametric}
\bibdata{tensor_wang.bib}
\bibcite{adomavicius2011context}{{1}{2011}{{Adomavicius and Tuzhilin}}{{}}}
\bibcite{allen2012sparse}{{2}{2012}{{Allen}}{{}}}
\bibcite{alon2016sign}{{3}{2016}{{Alon et~al.}}{{Alon, Moran, and Yehudayoff}}}
\bibcite{anandkumar2014tensor}{{4}{2014}{{Anandkumar et~al.}}{{Anandkumar, Ge, Hsu, Kakade, and Telgarsky}}}
\bibcite{anandkumar2017analyzing}{{5}{2017}{{Anandkumar et~al.}}{{Anandkumar, Ge, and Janzamin}}}
\bibcite{balabdaoui2019least}{{6}{2019}{{Balabdaoui et~al.}}{{Balabdaoui, Durot, and Jankowski}}}
\bibcite{balasubramanian2021nonparametric}{{7}{2021}{{Balasubramanian}}{{}}}
\bibcite{bartlett2006convexity}{{8}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{cai2019nonconvex}{{9}{2019}{{Cai et~al.}}{{Cai, Li, Poor, and Chen}}}
\bibcite{chan2014consistent}{{10}{2014}{{Chan and Airoldi}}{{}}}
\bibcite{chatterjee2015matrix}{{11}{2015}{{Chatterjee}}{{}}}
\bibcite{chi2020provable}{{12}{2020}{{Chi et~al.}}{{Chi, Gaines, Sun, Zhou, and Yang}}}
\bibcite{cohn2013fast}{{13}{2013}{{Cohn and Umans}}{{}}}
\bibcite{de2000multilinear}{{14}{2000}{{De~Lathauwer et~al.}}{{De~Lathauwer, De~Moor, and Vandewalle}}}
\bibcite{de2003nondeterministic}{{15}{2003}{{De~Wolf}}{{}}}
\bibcite{fan2019online}{{16}{2019}{{Fan and Udell}}{{}}}
\bibcite{ganti2017learning}{{17}{2017}{{Ganti et~al.}}{{Ganti, Rao, Balzano, Willett, and Nowak}}}
\bibcite{ganti2015matrix}{{18}{2015}{{Ganti et~al.}}{{Ganti, Balzano, and Willett}}}
\bibcite{gao2015rate}{{19}{2015}{{Gao et~al.}}{{Gao, Lu, and Zhou}}}
\bibcite{genzel2020robust}{{20}{2020}{{Genzel and Stollenwerk}}{{}}}
\bibcite{ghadermarzy2018learning}{{21}{2018}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{ghadermarzy2019near}{{22}{2019}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{globerson2007euclidean}{{23}{2007}{{Globerson et~al.}}{{Globerson, Chechik, Pereira, and Tishby}}}
\bibcite{han2020optimal}{{24}{2022}{{Han et~al.}}{{Han, Willett, and Zhang}}}
\bibcite{hao2019sparse}{{25}{2021}{{Hao et~al.}}{{Hao, Wang, Wang, Zhang, Yang, and Sun}}}
\bibcite{he2017kernelized}{{26}{2017}{{He et~al.}}{{He, Lu, Ma, Wang, Shen, Philip, and Ragin}}}
\bibcite{hillar2013most}{{27}{2013}{{Hillar and Lim}}{{}}}
\bibcite{hitchcock1927expression}{{28}{1927}{{Hitchcock}}{{}}}
\bibcite{hong2020generalized}{{29}{2020}{{Hong et~al.}}{{Hong, Kolda, and Duersch}}}
\bibcite{hore2016tensor}{{30}{2016}{{Hore et~al.}}{{Hore, Vi{\~n}uela, Buil, Knight, McCarthy, Small, and Marchini}}}
\bibcite{hu2021generalized}{{31}{2022}{{Hu et~al.}}{{Hu, Lee, and Wang}}}
\bibcite{jain2014provable}{{32}{2014}{{Jain and Oh}}{{}}}
\bibcite{jdanov2019human}{{33}{2019}{{Jdanov et~al.}}{{Jdanov, Jasilionis, Shkolnikov, and Barbieri}}}
\bibcite{klopp2017oracle}{{34}{2017}{{Klopp et~al.}}{{Klopp, Tsybakov, and Verzelen}}}
\bibcite{kolda2003counterexample}{{35}{2003}{{Kolda}}{{}}}
\bibcite{kolda2009tensor}{{36}{2009}{{Kolda and Bader}}{{}}}
\bibcite{kosorok2007introduction}{{37}{2007}{{Kosorok}}{{}}}
\bibcite{pmlr-v119-lee20i}{{38}{2020}{{Lee and Wang}}{{}}}
\bibcite{lee2021nonparametric}{{39}{2021}{{Lee et~al.}}{{Lee, Li, Zhang, and Wang}}}
\bibcite{li2009brain}{{40}{2009}{{Li et~al.}}{{Li, Liu, Li, Qin, Li, Yu, and Jiang}}}
\bibcite{lovasz2012large}{{41}{2012}{{Lov{\'a}sz}}{{}}}
\bibcite{montanari2018spectral}{{42}{2018}{{Montanari and Sun}}{{}}}
\bibcite{mu2014square}{{43}{2014}{{Mu et~al.}}{{Mu, Huang, Wright, and Goldfarb}}}
\bibcite{nickel2011three}{{44}{2011}{{Nickel et~al.}}{{Nickel, Tresp, and Kriegel}}}
\bibcite{pmlr-v70-ongie17a}{{45}{2017}{{Ongie et~al.}}{{Ongie, Willett, Nowak, and Balzano}}}
\bibcite{robinson1988root}{{46}{1988}{{Robinson}}{{}}}
\bibcite{scott2011surrogate}{{47}{2011}{{Scott}}{{}}}
\bibcite{shen1994convergence}{{48}{1994}{{Shen and Wong}}{{}}}
\bibcite{shen2003psi}{{49}{2003}{{Shen et~al.}}{{Shen, Tseng, Zhang, and Wong}}}
\bibcite{tsybakov2004optimal}{{50}{2004}{{Tsybakov}}{{}}}
\bibcite{wang2017bayesian}{{51}{2017}{{Wang et~al.}}{{Wang, Durante, Jung, and Dunson}}}
\bibcite{wang2019common}{{52}{2019}{{Wang et~al.}}{{Wang, Zhang, and Dunson}}}
\bibcite{wang2018learning}{{53}{2020}{{Wang and Li}}{{}}}
\bibcite{wang2019multiway}{{54}{2019}{{Wang and Zeng}}{{}}}
\bibcite{xu2018rates}{{55}{2018}{{Xu}}{{}}}
\bibcite{xu2020class}{{56}{2020}{{Xu et~al.}}{{Xu, Dan, Khim, and Ravikumar}}}
\bibcite{yuan2016tensor}{{57}{2016}{{Yuan and Zhang}}{{}}}
\bibcite{zhang2018tensor}{{58}{2018}{{Zhang and Xia}}{{}}}
\bibcite{zhang2018network}{{59}{2022}{{Zhang et~al.}}{{Zhang, Sun, and Li}}}
\bibcite{zhang2017estimating}{{60}{2017}{{Zhang et~al.}}{{Zhang, Levina, and Zhu}}}
\bibcite{zhang2019tensor}{{61}{2019}{{Zhang et~al.}}{{Zhang, Allen, Zhu, and Dunson}}}
\bibcite{zhao2015hypergraph}{{62}{2015}{{Zhao}}{{}}}
\bibcite{zhou2020broadcasted}{{63}{2020}{{Zhou et~al.}}{{Zhou, Wong, and He}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Experimental details in Section\nobreakspace  {}\ref  {sec:motivation}}{33}{section.1}}
\newlabel{sec:numericalrank}{{A}{33}{Acknowledgements}{section.1}{}}
\newlabel{eq:numeric}{{A}{34}{Acknowledgements}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proofs of theory in Section\nobreakspace  {}\ref  {sec:algebry}}{34}{section.2}}
\newlabel{sec:proofalg}{{B}{34}{Acknowledgements}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Proofs of Propositions\nobreakspace  {}\ref  {cor:monotonic}-\ref  {prop:global2}}{34}{subsection.2.1}}
\MT@newlabel{eq:sign-rank}
\newlabel{eq:support}{{25}{35}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{equation.2.25}{}}
\MT@newlabel{eq:support}
\newlabel{eq:indicator}{{26}{35}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{equation.2.26}{}}
\MT@newlabel{eq:indicator}
\newlabel{eq:sum}{{27}{35}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{equation.2.27}{}}
\MT@newlabel{eq:sum}
\newlabel{eq:matrix}{{28}{36}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{equation.2.28}{}}
\MT@newlabel{eq:matrix}
\newlabel{eq:entrywise}{{29}{36}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{equation.2.29}{}}
\MT@newlabel{eq:entrywise}
\newlabel{eq:A}{{30}{37}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{equation.2.30}{}}
\newlabel{eq:decrease}{{31}{37}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{equation.2.31}{}}
\MT@newlabel{eq:decrease}
\MT@newlabel{eq:A}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proofs of Theory in Section\nobreakspace  {}\ref  {sec:stat}}{37}{section.3}}
\newlabel{sec:proofs}{{C}{37}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global2}}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Proof of Theorem\nobreakspace  {}\ref  {prop:global}}{37}{subsection.3.1}}
\newlabel{eq:risk}{{32}{37}{Proof of Theorem~\ref {prop:global}}{equation.3.32}{}}
\newlabel{eq:I}{{33}{37}{Proof of Theorem~\ref {prop:global}}{equation.3.33}{}}
\MT@newlabel{eq:I}
\MT@newlabel{eq:I}
\MT@newlabel{eq:risk}
\newlabel{eq:minimum}{{34}{37}{Proof of Theorem~\ref {prop:global}}{equation.3.34}{}}
\MT@newlabel{eq:minimum}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Proof of Theorem\nobreakspace  {}\ref  {thm:population}}{38}{subsection.3.2}}
\MT@newlabel{eq:minimum}
\newlabel{eq:population2}{{35}{38}{Proof of Theorem~\ref {thm:population}}{equation.3.35}{}}
\newlabel{eq:ass}{{36}{38}{Proof of Theorem~\ref {thm:population}}{equation.3.36}{}}
\MT@newlabel{eq:population2}
\newlabel{eq:1}{{37}{38}{Proof of Theorem~\ref {thm:population}}{equation.3.37}{}}
\MT@newlabel{eq:ass}
\MT@newlabel{eq:1}
\MT@newlabel{eq:1}
\newlabel{eq:MAE}{{C.2}{38}{Proof of Theorem~\ref {thm:population}}{equation.3.37}{}}
\MT@newlabel{eq:1}
\newlabel{eq:2}{{38}{38}{Proof of Theorem~\ref {thm:population}}{equation.3.38}{}}
\MT@newlabel{eq:2}
\newlabel{eq:rmk}{{14}{39}{}{rmk.14}{}}
\newlabel{eq:remark}{{39}{39}{}{equation.3.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Proof of Theorem\nobreakspace  {}\ref  {prop:global2}}{39}{subsection.3.3}}
\newlabel{eq:risk}{{40}{39}{Proof of Theorem~\ref {prop:global2}}{equation.3.40}{}}
\newlabel{eq:I_q}{{C.3}{39}{Proof of Theorem~\ref {prop:global2}}{equation.3.40}{}}
\newlabel{eq:oddt}{{C.3}{39}{Proof of Theorem~\ref {prop:global2}}{equation.3.40}{}}
\newlabel{eq:odd}{{41}{39}{Proof of Theorem~\ref {prop:global2}}{equation.3.41}{}}
\MT@newlabel{eq:odd}
\citation{shen1994convergence}
\newlabel{eq:event}{{42}{40}{Proof of Theorem~\ref {prop:global2}}{equation.3.42}{}}
\newlabel{eq:even}{{C.3}{40}{Proof of Theorem~\ref {prop:global2}}{equation.3.42}{}}
\MT@newlabel{eq:event}
\@writefile{toc}{\contentsline {section}{\numberline {D}Proof of Theory in Section\nobreakspace  {}\ref  {sec:estimation}}{40}{section.4}}
\newlabel{sec:novalty}{{D}{40}{Proof of Theorem~\ref {prop:global2}}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Proof of sign tensor estimation}{40}{subsection.4.1}}
\newlabel{lem:variance}{{1}{40}{Variance-to-mean relationship}{lem.1}{}}
\newlabel{eq:sample2}{{43}{41}{Variance-to-mean relationship}{equation.4.43}{}}
\newlabel{eq:variance}{{44}{41}{Variance-to-mean relationship}{equation.4.44}{}}
\newlabel{eq:mae}{{45}{41}{Proof of sign tensor estimation}{equation.4.45}{}}
\MT@newlabel{eq:variance}
\MT@newlabel{eq:mae}
\MT@newlabel{eq:bound}
\newlabel{eq:opt}{{46}{41}{Proof of sign tensor estimation}{equation.4.46}{}}
\MT@newlabel{eq:sample2}
\newlabel{eq:empirical}{{D.1}{41}{Proof of sign tensor estimation}{equation.4.46}{}}
\MT@newlabel{eq:opt}
\newlabel{eq:unionpb}{{47}{42}{Proof of sign tensor estimation}{equation.4.47}{}}
\MT@newlabel{eq:unionpb}
\newlabel{eq:vn}{{48}{42}{Proof of sign tensor estimation}{equation.4.48}{}}
\MT@newlabel{eq:unionpb}
\MT@newlabel{eq:vn}
\newlabel{eq:gammabd}{{49}{42}{Proof of sign tensor estimation}{equation.4.49}{}}
\MT@newlabel{eq:gammabd}
\newlabel{eq:gammabd2}{{50}{42}{Proof of sign tensor estimation}{equation.4.50}{}}
\newlabel{eq:equation}{{51}{43}{Proof of sign tensor estimation}{equation.4.51}{}}
\MT@newlabel{eq:equation}
\MT@newlabel{eq:equation}
\newlabel{eq:delta}{{D.1}{43}{Proof of sign tensor estimation}{equation.4.51}{}}
\MT@newlabel{eq:gammabd2}
\MT@newlabel{eq:remark}
\newlabel{eq:final}{{52}{43}{Proof of sign tensor estimation}{equation.4.52}{}}
\MT@newlabel{eq:final}
\newlabel{pro:inftynorm}{{4}{43}{Bracketing number}{defn.4}{}}
\citation{kosorok2007introduction}
\citation{mu2014square}
\newlabel{lem:metric}{{2}{44}{Bracketing complexity of low-rank tensors}{lem.2}{}}
\newlabel{eq:specification}{{2}{44}{Bracketing complexity of low-rank tensors}{lem.2}{}}
\newlabel{eq:L}{{53}{44}{Bracketing complexity of low-rank tensors}{equation.4.53}{}}
\MT@newlabel{eq:L}
\newlabel{eq:complexity}{{54}{44}{Proof of sign tensor estimation}{equation.4.54}{}}
\MT@newlabel{eq:complexity}
\newlabel{eq:g}{{55}{44}{Proof of sign tensor estimation}{equation.4.55}{}}
\citation{shen1994convergence}
\MT@newlabel{eq:L}
\MT@newlabel{eq:g}
\newlabel{thm:refer}{{6}{45}{Theorem 3 in~\cite {shen1994convergence}}{thm.6}{}}
\newlabel{eq:oneside}{{6}{45}{Theorem 3 in~\cite {shen1994convergence}}{thm.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Proof of signal estimation error}{45}{subsection.4.2}}
\newlabel{sec:proofsignal}{{D.2}{45}{Proof of signal estimation error}{subsection.4.2}{}}
\MT@newlabel{eq:bound2}
\newlabel{eq:pfmain3}{{56}{45}{Proof of signal estimation error}{equation.4.56}{}}
\MT@newlabel{eq:pfmain3}
\newlabel{eq:total}{{57}{46}{Proof of signal estimation error}{equation.4.57}{}}
\MT@newlabel{eq:bound}
\MT@newlabel{eq:total}
\newlabel{eq:twobounds}{{58}{46}{Proof of signal estimation error}{equation.4.58}{}}
\MT@newlabel{eq:twobounds}
\MT@newlabel{eq:total}
\newlabel{eq:pbAf}{{59}{46}{Proof of signal estimation error}{equation.4.59}{}}
\MT@newlabel{eq:bound}
\newlabel{eq:pbA}{{60}{47}{Proof of signal estimation error}{equation.4.60}{}}
\MT@newlabel{eq:pbA}
\MT@newlabel{eq:pbAf}
\MT@newlabel{eq:pbA}
\newlabel{lem:H}{{3}{47}{}{lem.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Proof of Corollary\nobreakspace  {}\ref  {cor:completion}}{47}{subsection.4.3}}
\MT@newlabel{eq:bound2}
\@writefile{toc}{\contentsline {section}{\numberline {E}Proofs of Theory in Section\nobreakspace  {}\ref  {sec:extension}}{47}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.1}Proof of Corollary\nobreakspace  {}\ref  {thm:unbddno1}}{47}{subsection.5.1}}
\newlabel{eq:corosign}{{61}{47}{Proof of Corollary~\ref {thm:unbddno1}}{equation.5.61}{}}
\newlabel{eq:variance2}{{62}{48}{Proof of Corollary~\ref {thm:unbddno1}}{equation.5.62}{}}
\MT@newlabel{eq:variance2}
\newlabel{eq:vartomean}{{63}{48}{Proof of Corollary~\ref {thm:unbddno1}}{equation.5.63}{}}
\MT@newlabel{eq:vartomean}
\newlabel{eq:empriskbd}{{64}{48}{Proof of Corollary~\ref {thm:unbddno1}}{equation.5.64}{}}
\newlabel{eq:subgbd}{{65}{48}{Proof of Corollary~\ref {thm:unbddno1}}{equation.5.65}{}}
\MT@newlabel{eq:empriskbd}
\MT@newlabel{eq:subgbd}
\newlabel{eq:riskunbd}{{66}{48}{Proof of Corollary~\ref {thm:unbddno1}}{equation.5.66}{}}
\MT@newlabel{eq:riskunbd}
\MT@newlabel{eq:final}
\MT@newlabel{eq:corosign}
\newlabel{lem:subg}{{4}{49}{sub-Gaussian maximum}{lem.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.2}Proofs of Corollary\nobreakspace  {}\ref  {thm:bernoulli} and Corollary\nobreakspace  {}\ref  {thm:binom}}{49}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E.3}Proof of Theorem\nobreakspace  {}\ref  {thm:extension}}{49}{subsection.5.3}}
\newlabel{eq:largemgopt}{{67}{49}{Proof of Theorem~\ref {thm:extension}}{equation.5.67}{}}
\newlabel{eq:sfe}{{68}{49}{Proof of Theorem~\ref {thm:extension}}{equation.5.68}{}}
\MT@newlabel{eq:rfs}
\MT@newlabel{eq:sfe}
\MT@newlabel{eq:sfe}
\newlabel{eq:empirical}{{E.3}{49}{Proof of Theorem~\ref {thm:extension}}{equation.5.68}{}}
\MT@newlabel{eq:largemgopt}
\newlabel{eq:unionpb2}{{69}{50}{Proof of Theorem~\ref {thm:extension}}{equation.5.69}{}}
\MT@newlabel{eq:unionpb2}
\MT@newlabel{eq:unionpb2}
\newlabel{eq:vn2}{{70}{50}{Proof of Theorem~\ref {thm:extension}}{equation.5.70}{}}
\citation{lee2021nonparametric}
\MT@newlabel{eq:unionpb2}
\MT@newlabel{eq:vn2}
\newlabel{eq:gammabd3}{{71}{51}{Proof of Theorem~\ref {thm:extension}}{equation.5.71}{}}
\newlabel{eq:cd}{{72}{51}{Proof of Theorem~\ref {thm:extension}}{equation.5.72}{}}
\MT@newlabel{eq:cd}
\newlabel{eq:choice}{{73}{51}{Proof of Theorem~\ref {thm:extension}}{equation.5.73}{}}
\MT@newlabel{eq:choice}
\MT@newlabel{eq:gammabd3}
\newlabel{eq:gammabd4}{{E.3}{51}{Proof of Theorem~\ref {thm:extension}}{equation.5.73}{}}
\MT@newlabel{eq:choice}
\MT@newlabel{eq:final}
