\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{anandkumar2014tensor}
\citation{wang2017bayesian}
\citation{hore2016tensor}
\citation{jain2014provable,montanari2018spectral}
\citation{hitchcock1927expression}
\citation{de2000multilinear}
\citation{wang2019multiway}
\citation{ghadermarzy2018learning}
\citation{hore2016tensor}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{eq:modelintro}{{1}{1}{Introduction}{equation.1.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:modelintro}
\citation{chan2014consistent}
\citation{anandkumar2014tensor,montanari2018spectral,cai2019nonconvex}
\citation{wang2018learning,han2020optimal,ghadermarzy2018learning}
\citation{xu2018rates,zhang2017estimating,chan2014consistent}
\citation{ganti2015matrix}
\citation{pmlr-v70-ongie17a,fan2019online}
\citation{lee2021nonparametric}
\citation{anandkumar2017analyzing}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Numerical tensor rank of $\Theta $ vs.\ transformation level $c$ in the first example. (b) Top $d=30$ tensor singular values in the second example. \relax }}{2}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{2}{(a) Numerical tensor rank of $\Theta $ vs.\ transformation level $c$ in the first example. (b) Top $d=30$ tensor singular values in the second example. \relax }{figure.caption.2}{}}
\citation{hitchcock1927expression}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model and proposal overview}{3}{section.2}\protected@file@percent }
\newlabel{sec:overview}{{2}{3}{Model and proposal overview}{section.2}{}}
\newlabel{eq:model}{{2}{3}{Model and proposal overview}{equation.2.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:intuition}{{3}{3}{Model and proposal overview}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Oracle properties of sign representable tensors}{3}{section.3}\protected@file@percent }
\newlabel{sec:representation}{{3}{3}{Oracle properties of sign representable tensors}{section.3}{}}
\MT@newlabel{eq:model}
\citation{cohn2013fast}
\citation{alon2016sign}
\citation{de2003nondeterministic}
\citation{hillar2013most}
\citation{alon2016sign}
\citation{kolda2009tensor}
\citation{wang2019multiway,chi2020provable}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of our method in the context of an order-2 tensor (a.k.a.\ matrix). (a): a noisy, incomplete tensor input. (b)-(c): estimation of sign tensor series $\textup  {sgn}(\Theta -\pi )$ for $\pi \in $ {\relax \fontsize  {9}{10}\selectfont  $\{-1,\ldots  ,-{1/ H},0,{1/H},\ldots  ,1\}$}. (d): recovered signal $\mathaccentV {hat}05E\Theta $. The depicted signal is a full-rank matrix based on Example\nobreakspace  {}\ref  {eq:example} in Section\nobreakspace  {}\ref  {sec:representation}.\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:demo}{{2}{4}{Illustration of our method in the context of an order-2 tensor (a.k.a.\ matrix). (a): a noisy, incomplete tensor input. (b)-(c): estimation of sign tensor series $\sign (\Theta -\pi )$ for $\pi \in $ {\footnotesize $\{-1,\ldots ,-{1/ H},0,{1/H},\ldots ,1\}$}. (d): recovered signal $\hat \Theta $. The depicted signal is a full-rank matrix based on Example~\ref {eq:example} in Section~\ref {sec:representation}.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sign-rank and sign tensor series}{4}{subsection.3.1}\protected@file@percent }
\newlabel{sec:sign-rank}{{3.1}{4}{Sign-rank and sign tensor series}{subsection.3.1}{}}
\newlabel{cor:monotonic}{{1}{4}{Upper bounds of the sign-rank}{prop.1}{}}
\citation{wang2018learning}
\citation{hong2020generalized}
\citation{robinson1988root}
\citation{balabdaoui2019least,ganti2017learning}
\citation{zhao2015hypergraph,lovasz2006limits}
\newlabel{eq:example}{{5}{5}{Structured tensors with repeating entries}{example.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Statistical characterization of sign tensors via weighted classification}{5}{subsection.3.2}\protected@file@percent }
\newlabel{sec:identifiability}{{3.2}{5}{Statistical characterization of sign tensors via weighted classification}{subsection.3.2}{}}
\MT@newlabel{eq:intuition}
\newlabel{eq:sample}{{4}{5}{Statistical characterization of sign tensors via weighted classification}{equation.3.4}{}}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:sample}
\newlabel{eq:population}{{5}{5}{Statistical characterization of sign tensors via weighted classification}{equation.3.5}{}}
\MT@newlabel{eq:model}
\newlabel{prop:global}{{2}{5}{Global optimum of weighted risk}{prop.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:optimal}{{2}{5}{Global optimum of weighted risk}{prop.2}{}}
\citation{lee2021nonparametric}
\citation{hastie2009elements}
\newlabel{ass:margin}{{1}{6}{$\alpha $-smoothness}{assumption.1}{}}
\newlabel{eq:smooth}{{6}{6}{$\alpha $-smoothness}{equation.3.6}{}}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\newlabel{thm:population}{{1}{6}{Identifiability}{thm.1}{}}
\MT@newlabel{eq:population}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:smooth}
\@writefile{toc}{\contentsline {section}{\numberline {4}Nonparametric tensor completion via sign series}{6}{section.4}\protected@file@percent }
\newlabel{sec:estimation}{{4}{6}{Nonparametric tensor completion via sign series}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Statistical error and sample complexity}{6}{subsection.4.1}\protected@file@percent }
\newlabel{sec:error}{{4.1}{6}{Statistical error and sample complexity}{subsection.4.1}{}}
\MT@newlabel{eq:model}
\newlabel{eq:est}{{7}{6}{Statistical error and sample complexity}{equation.4.7}{}}
\citation{wang2019multiway}
\citation{zhang2018tensor,wang2018learning,hu2021generalized}
\citation{xu2018rates}
\citation{ganti2015matrix}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:est}
\newlabel{thm:classification}{{2}{7}{Sign tensor estimation}{thm.2}{}}
\MT@newlabel{eq:est}
\newlabel{eq:bound}{{8}{7}{Sign tensor estimation}{equation.4.8}{}}
\newlabel{thm:estimation}{{3}{7}{Tensor estimation error}{thm.3}{}}
\MT@newlabel{eq:est}
\newlabel{eq:bound2}{{9}{7}{Tensor estimation error}{equation.4.9}{}}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:bound}
\citation{wang2019multiway}
\citation{ganti2015matrix}
\citation{zhang2018tensor,wang2018learning,hu2021generalized}
\citation{yuan2016tensor,ghadermarzy2019near,pmlr-v119-lee20i}
\citation{ghadermarzy2018learning,wang2018learning,hong2020generalized,alquier2019estimation}
\citation{bartlett2006convexity}
\citation{ghadermarzy2018learning,wang2018learning,hong2020generalized,alquier2019estimation}
\citation{alquier2019estimation,genzel2020robust,he2017kernelized}
\citation{scott2011surrogate}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of our statistical rates compared to existing works under different models. For notational simplicity, we present error rates assuming equal tensor dimension in all modes and finite $|\mathcal  {N}|$ for the smooth tensor model. Here $K$ denotes tensor order and $d$ denotes tensor dimension. \relax }}{8}{table.caption.4}\protected@file@percent }
\newlabel{tb:comparison}{{1}{8}{Summary of our statistical rates compared to existing works under different models. For notational simplicity, we present error rates assuming equal tensor dimension in all modes and finite $|\tN |$ for the smooth tensor model. Here $K$ denotes tensor order and $d$ denotes tensor dimension.\\\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Implementation via learning reduction}{8}{subsection.4.2}\protected@file@percent }
\MT@newlabel{eq:est}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Nonparametric tensor completion via learning reduction\relax }}{8}{algorithm.1}\protected@file@percent }
\newlabel{alg:tensorT}{{1}{8}{Nonparametric tensor completion via learning reduction\relax }{algorithm.1}{}}
\MT@newlabel{eq:sample}
\citation{wang2018learning,han2020optimal}
\citation{bartlett2006convexity}
\citation{ghadermarzy2018learning,wang2018learning}
\citation{wang2017bayesian}
\citation{globerson2007euclidean}
\MT@newlabel{eq:population}
\newlabel{thm:hinge}{{4}{9}{Large-margin loss}{thm.4}{}}
\MT@newlabel{eq:bound2}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical experiments}{9}{section.5}\protected@file@percent }
\newlabel{sec:simulation}{{5}{9}{Numerical experiments}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Simulation models used for comparison. Here $\bm  {M}_k\in \{0,1\}^{d\times 3}$ denotes membership matrix; $\mathcal  {C}\in \mathbb  {R}^{3\times 3\times 3}$ is the block mean tensor; $\bm  {a}=d^{-1}(1,2,\ldots  ,d)^T$ is a length-$d$ vector; $\mathcal  {Z}_{\qopname  \relax m{max}}$ and $\mathcal  {Z}_{\qopname  \relax m{min}}$ are order-3 tensors with entries $d^{-1}\qopname  \relax m{max}(i,j,k)$ and $d^{-1}\qopname  \relax m{min}(i,j,k)$, respectively. \relax }}{9}{table.caption.5}\protected@file@percent }
\newlabel{tab:simulation}{{2}{9}{Simulation models used for comparison. Here $\mM _k\in \{0,1\}^{d\times 3}$ denotes membership matrix; $\tC \in \mathbb {R}^{3\times 3\times 3}$ is the block mean tensor; $\ma =d^{-1}(1,2,\ldots ,d)^T$ is a length-$d$ vector; $\tZ _{\max }$ and $\tZ _{\min }$ are order-3 tensors with entries $d^{-1}\max (i,j,k)$ and $d^{-1}\min (i,j,k)$, respectively.\\\relax }{table.caption.5}{}}
\citation{li2009brain,wang2017bayesian}
\bibstyle{plain}
\bibdata{tensor_wang}
\bibcite{alon2016sign}{{1}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance comparison between different methods. (a)-(b): Estimation error versus tensor dimension. (c)-(d): Estimation error versus observation fraction. Panels (a) and (c) are for model 2, whereas (b) and (d) are for model 3.\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:compare1}{{3}{10}{Performance comparison between different methods. (a)-(b): Estimation error versus tensor dimension. (c)-(d): Estimation error versus observation fraction. Panels (a) and (c) are for model 2, whereas (b) and (d) are for model 3.\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces MAE comparison between {\bf  NonparaT} ($H=20$) and {\bf  CPT} in the real data analysis. Standard errors are in parenthesis.\relax }}{10}{table.3}\protected@file@percent }
\newlabel{tab:data}{{3}{10}{MAE comparison between {\bf NonparaT} ($H=20$) and {\bf CPT} in the real data analysis. Standard errors are in parenthesis.\relax }{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces (a) top IQ-associated edges in the brain connectivity data. (b) top (authors, words, year) triplets in the NIPS data. \relax }}{10}{figure.4}\protected@file@percent }
\newlabel{fig:signal}{{4}{10}{(a) top IQ-associated edges in the brain connectivity data. (b) top (authors, words, year) triplets in the NIPS data. \relax }{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{10}{section.6}\protected@file@percent }
\bibcite{alquier2019estimation}{{2}{}{{}}{{}}}
\bibcite{anandkumar2014tensor}{{3}{}{{}}{{}}}
\bibcite{anandkumar2017analyzing}{{4}{}{{}}{{}}}
\bibcite{balabdaoui2019least}{{5}{}{{}}{{}}}
\bibcite{bartlett2006convexity}{{6}{}{{}}{{}}}
\bibcite{cai2019nonconvex}{{7}{}{{}}{{}}}
\bibcite{chan2014consistent}{{8}{}{{}}{{}}}
\bibcite{chi2020provable}{{9}{}{{}}{{}}}
\bibcite{cohn2013fast}{{10}{}{{}}{{}}}
\bibcite{de2000multilinear}{{11}{}{{}}{{}}}
\bibcite{de2003nondeterministic}{{12}{}{{}}{{}}}
\bibcite{fan2019online}{{13}{}{{}}{{}}}
\bibcite{ganti2017learning}{{14}{}{{}}{{}}}
\bibcite{ganti2015matrix}{{15}{}{{}}{{}}}
\bibcite{genzel2020robust}{{16}{}{{}}{{}}}
\bibcite{ghadermarzy2018learning}{{17}{}{{}}{{}}}
\bibcite{ghadermarzy2019near}{{18}{}{{}}{{}}}
\bibcite{globerson2007euclidean}{{19}{}{{}}{{}}}
\bibcite{han2020optimal}{{20}{}{{}}{{}}}
\bibcite{hastie2009elements}{{21}{}{{}}{{}}}
\bibcite{he2017kernelized}{{22}{}{{}}{{}}}
\bibcite{hillar2013most}{{23}{}{{}}{{}}}
\bibcite{hitchcock1927expression}{{24}{}{{}}{{}}}
\bibcite{hong2020generalized}{{25}{}{{}}{{}}}
\bibcite{hore2016tensor}{{26}{}{{}}{{}}}
\bibcite{hu2021generalized}{{27}{}{{}}{{}}}
\bibcite{jain2014provable}{{28}{}{{}}{{}}}
\bibcite{kolda2009tensor}{{29}{}{{}}{{}}}
\bibcite{lee2021nonparametric}{{30}{}{{}}{{}}}
\bibcite{pmlr-v119-lee20i}{{31}{}{{}}{{}}}
\bibcite{li2009brain}{{32}{}{{}}{{}}}
\bibcite{lovasz2006limits}{{33}{}{{}}{{}}}
\bibcite{montanari2018spectral}{{34}{}{{}}{{}}}
\bibcite{pmlr-v70-ongie17a}{{35}{}{{}}{{}}}
\bibcite{robinson1988root}{{36}{}{{}}{{}}}
\bibcite{scott2011surrogate}{{37}{}{{}}{{}}}
\bibcite{wang2017bayesian}{{38}{}{{}}{{}}}
\bibcite{wang2018learning}{{39}{}{{}}{{}}}
\bibcite{wang2019multiway}{{40}{}{{}}{{}}}
\bibcite{xu2018rates}{{41}{}{{}}{{}}}
\bibcite{yuan2016tensor}{{42}{}{{}}{{}}}
\bibcite{zhang2018tensor}{{43}{}{{}}{{}}}
\bibcite{zhang2017estimating}{{44}{}{{}}{{}}}
\bibcite{zhao2015hypergraph}{{45}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
