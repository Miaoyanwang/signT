\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{firstpage}{{}{1}{}{Doc-Start}{}}
\citation{anandkumar2014tensor}
\citation{wang2017bayesian}
\citation{hore2016tensor}
\citation{jain2014provable,montanari2018spectral}
\citation{hitchcock1927expression}
\citation{de2000multilinear}
\citation{wang2019multiway}
\citation{ghadermarzy2018learning}
\citation{hore2016tensor}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{eq:modelintro}{{1}{1}{Introduction}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Inadequacies of low-rank models}{1}{subsection.1.1}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:modelintro}
\citation{chan2014consistent}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Numerical rank of $\Theta $ versus $c$ in the first example. (b) Top $d=30$ tensor singular values in the second example. \relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{2}{(a) Numerical rank of $\Theta $ versus $c$ in the first example. (b) Top $d=30$ tensor singular values in the second example. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Our contributions}{2}{subsection.1.2}}
\citation{anandkumar2014tensor,montanari2018spectral,cai2019nonconvex}
\citation{ghadermarzy2018learning,wang2018learning,han2020optimal}
\citation{ganti2015matrix}
\citation{pmlr-v70-ongie17a,fan2019online}
\citation{anandkumar2017analyzing}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of our method. For visualization purpose, we plot an order-2 tensor (a.k.a.\ matrix); similar procedure applies to higher-order tensors. (a): a noisy and incomplete tensor input. (b) and (c): main steps of estimating sign tensor series $\textup  {sgn}(\Theta -\pi )$ for $\pi \in \{-1,\ldots  ,-{1\over H},0,{1\over H},\ldots  ,1\}$. (d) estimated signal $\mathaccentV {hat}05E\Theta $. The depicted signal is a full-rank matrix based on Example\nobreakspace  {}\ref  {eq:example} in Section\nobreakspace  {}\ref  {sec:representation}.\relax }}{3}{figure.caption.2}}
\newlabel{fig:demo}{{2}{3}{Illustration of our method. For visualization purpose, we plot an order-2 tensor (a.k.a.\ matrix); similar procedure applies to higher-order tensors. (a): a noisy and incomplete tensor input. (b) and (c): main steps of estimating sign tensor series $\sign (\Theta -\pi )$ for $\pi \in \{-1,\ldots ,-{1\over H},0,{1\over H},\ldots ,1\}$. (d) estimated signal $\hat \Theta $. The depicted signal is a full-rank matrix based on Example~\ref {eq:example} in Section~\ref {sec:representation}.\relax }{figure.caption.2}{}}
\citation{hitchcock1927expression}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Notation}{4}{subsection.1.3}}
\newlabel{eq:CP}{{2}{4}{Notation}{equation.1.2}{}}
\MT@newlabel{eq:CP}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model and proposal overview}{4}{section.2}}
\newlabel{sec:overview}{{2}{4}{Model and proposal overview}{section.2}{}}
\newlabel{eq:model}{{3}{4}{Model and proposal overview}{equation.2.3}{}}
\MT@newlabel{eq:model}
\citation{cohn2013fast}
\citation{alon2016sign}
\citation{de2003nondeterministic}
\citation{hillar2013most}
\citation{alon2016sign}
\@writefile{toc}{\contentsline {section}{\numberline {3}Statistical properties of sign representable tensors}{6}{section.3}}
\newlabel{sec:representation}{{3}{6}{Statistical properties of sign representable tensors}{section.3}{}}
\MT@newlabel{eq:model}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sign-rank and sign tensor series}{6}{subsection.3.1}}
\newlabel{sec:sign-rank}{{3.1}{6}{Sign-rank and sign tensor series}{subsection.3.1}{}}
\citation{kolda2009tensor}
\newlabel{cor:monotonic}{{1}{7}{Sign-rank and sign tensor series}{prop.1}{}}
\newlabel{prop:extention}{{2}{7}{Sign-rank and sign tensor series}{prop.2}{}}
\newlabel{cor:broadness}{{2}{7}{Sign-rank and sign tensor series}{prop.2}{}}
\MT@newlabel{eq:model}
\citation{wang2019multiway,chi2020provable}
\citation{wang2018learning}
\citation{hong2020generalized}
\citation{robinson1988root}
\citation{balabdaoui2019least,ganti2017learning}
\citation{chan2014consistent,xu2018rates}
\newlabel{eq:example}{{5}{8}{Sign-rank and sign tensor series}{example.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Statistical characterization of sign tensors via weighted classification}{9}{subsection.3.2}}
\newlabel{sec:identifiability}{{3.2}{9}{Statistical characterization of sign tensors via weighted classification}{subsection.3.2}{}}
\newlabel{eq:sample}{{4}{9}{Statistical characterization of sign tensors via weighted classification}{equation.3.4}{}}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:sample}
\newlabel{eq:population}{{5}{9}{Statistical characterization of sign tensors via weighted classification}{equation.3.5}{}}
\MT@newlabel{eq:model}
\newlabel{prop:global}{{3}{9}{Statistical characterization of sign tensors via weighted classification}{prop.3}{}}
\MT@newlabel{eq:model}
\newlabel{eq:optimal}{{3}{9}{Statistical characterization of sign tensors via weighted classification}{prop.3}{}}
\newlabel{ass:margin}{{1}{10}{Statistical characterization of sign tensors via weighted classification}{assumption.1}{}}
\newlabel{eq:smooth}{{6}{10}{Statistical characterization of sign tensors via weighted classification}{equation.3.6}{}}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Three examples of CDF, $G(\pi )=\mathbb  {P}_{\bm  {X}}(f(\bm  {X})\leq \pi )$, and the associated smoothness index $\alpha $. (a) and (b) $G(\pi )$ with $\alpha =1$ because the function $G(\pi )$ induces bounded pseudo density in the range of $\pi $. (c) $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue) except for a few jump points with $\alpha =0$ (in red). The dashed lines correspond to local $(\alpha ,\pi )$-smoothness.\relax }}{11}{figure.caption.3}}
\newlabel{fig:demo}{{3}{11}{Three examples of CDF, $G(\pi )=\mathbb {P}_{\mX }(f(\mX )\leq \pi )$, and the associated smoothness index $\alpha $. (a) and (b) $G(\pi )$ with $\alpha =1$ because the function $G(\pi )$ induces bounded pseudo density in the range of $\pi $. (c) $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue) except for a few jump points with $\alpha =0$ (in red). The dashed lines correspond to local $(\alpha ,\pi )$-smoothness.\relax }{figure.caption.3}{}}
\newlabel{fig:CDF}{{3}{11}{Three examples of CDF, $G(\pi )=\mathbb {P}_{\mX }(f(\mX )\leq \pi )$, and the associated smoothness index $\alpha $. (a) and (b) $G(\pi )$ with $\alpha =1$ because the function $G(\pi )$ induces bounded pseudo density in the range of $\pi $. (c) $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue) except for a few jump points with $\alpha =0$ (in red). The dashed lines correspond to local $(\alpha ,\pi )$-smoothness.\relax }{figure.caption.3}{}}
\newlabel{thm:population}{{1}{11}{Statistical characterization of sign tensors via weighted classification}{thm.1}{}}
\MT@newlabel{eq:population}
\MT@newlabel{eq:sample}
\citation{hastie2009elements}
\MT@newlabel{eq:smooth}
\@writefile{toc}{\contentsline {section}{\numberline {4}Nonparametric tensor completion via sign series}{12}{section.4}}
\newlabel{sec:estimation}{{4}{12}{Nonparametric tensor completion via sign series}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Estimation error and sample complexity}{12}{subsection.4.1}}
\MT@newlabel{eq:model}
\newlabel{eq:est}{{7}{12}{Estimation error and sample complexity}{equation.4.7}{}}
\newlabel{eq:estimate}{{8}{12}{Estimation error and sample complexity}{equation.4.8}{}}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:est}
\newlabel{thm:classification}{{2}{12}{Estimation error and sample complexity}{thm.2}{}}
\MT@newlabel{eq:estimate}
\newlabel{eq:bound}{{9}{13}{Estimation error and sample complexity}{equation.4.9}{}}
\newlabel{thm:estimation}{{3}{13}{Estimation error and sample complexity}{thm.3}{}}
\MT@newlabel{eq:est}
\newlabel{eq:bound2}{{10}{13}{Estimation error and sample complexity}{equation.4.10}{}}
\newlabel{eq:real}{{11}{13}{Estimation error and sample complexity}{equation.4.11}{}}
\MT@newlabel{eq:bound2}
\MT@newlabel{eq:real}
\MT@newlabel{eq:bound}
\citation{wang2019multiway}
\citation{zhang2018tensor,wang2018learning}
\citation{ganti2015matrix}
\citation{yuan2016tensor,ghadermarzy2019near,pmlr-v119-lee20i}
\citation{ghadermarzy2018learning,wang2018learning,hong2020generalized}
\citation{bartlett2006convexity}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Numerical implementation}{15}{subsection.4.2}}
\MT@newlabel{eq:est}
\MT@newlabel{eq:estimate}
\citation{ghadermarzy2018learning}
\citation{wang2018learning,han2020optimal}
\citation{anandkumar2014tensor,hong2020generalized}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The full procedure of algorithm for nonparametric tensor completion.\relax }}{16}{figure.caption.4}}
\newlabel{fig:alg}{{4}{16}{The full procedure of algorithm for nonparametric tensor completion.\relax }{figure.caption.4}{}}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:CP}
\MT@newlabel{eq:est}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulations}{17}{section.5}}
\newlabel{sec:simulation}{{5}{17}{Simulations}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Simulation models used for comparison. We use $\bm  {M}_k\in \{0,1\}^{d\times 3}$ to denote membership matrices, $\mathcal  {C}\in \mathbb  {R}^{3\times 3\times 3}$ the block means, $\bm  {a}={1\over d}(1,2,\ldots  ,d)^T \in \mathbb  {R}^d$, $\mathcal  {Z}_{\qopname  \relax m{max}}$ and $\mathcal  {Z}_{\qopname  \relax m{min}}$ are order-3 tensors with entries ${1\over d}\qopname  \relax m{max}(i,j,k)$ and ${1\over d}\qopname  \relax m{min}(i,j,k)$, respectively.\relax }}{17}{table.caption.5}}
\newlabel{tab:simulation}{{1}{17}{Simulation models used for comparison. We use $\mM _k\in \{0,1\}^{d\times 3}$ to denote membership matrices, $\tC \in \mathbb {R}^{3\times 3\times 3}$ the block means, $\ma ={1\over d}(1,2,\ldots ,d)^T \in \mathbb {R}^d$, $\tZ _{\max }$ and $\tZ _{\min }$ are order-3 tensors with entries ${1\over d}\max (i,j,k)$ and ${1\over d}\min (i,j,k)$, respectively.\relax }{table.caption.5}{}}
\citation{wang2017bayesian}
\citation{globerson2007euclidean}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Estimation error versus tensor dimension. Panels (a)-(d) correspond to simulation models 1-4 in Table\nobreakspace  {}\ref  {tab:simulation}.\relax }}{18}{figure.caption.6}}
\newlabel{fig:compare1}{{5}{18}{Estimation error versus tensor dimension. Panels (a)-(d) correspond to simulation models 1-4 in Table~\ref {tab:simulation}.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Completion error versus observation fraction. Panels (a)-(d) correspond to simulation models 1-4 in Table\nobreakspace  {}\ref  {tab:simulation}. \relax }}{18}{figure.caption.7}}
\newlabel{fig:compare2}{{6}{18}{Completion error versus observation fraction. Panels (a)-(d) correspond to simulation models 1-4 in Table~\ref {tab:simulation}. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Data applications}{18}{section.6}}
\citation{li2009brain,wang2017bayesian}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MAE comparison in the brain data and NIPS data analysis. Reported MAEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Bold numbers indicate the minimal MAE among three methods. For low-rank CPT, we use R function {\tt  rTensor} with default hyperparameters, and for our method, we set $H=20$.\relax }}{19}{table.caption.8}}
\newlabel{tab:data}{{2}{19}{MAE comparison in the brain data and NIPS data analysis. Reported MAEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Bold numbers indicate the minimal MAE among three methods. For low-rank CPT, we use R function {\tt rTensor} with default hyperparameters, and for our method, we set $H=20$.\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Estimated signal tensors in the data analysis. (a) top edges associated with IQ scores in the brain connectivity data. The color indicates the estimated IQ effect size. (b) top authors and words for years 1996-1999 in the NIPS data. Authors and words are ranked by marginal averages based on $\mathaccentV {hat}05E\Theta $, where the marginal average is denoted in the parentheses. \relax }}{20}{figure.caption.9}}
\newlabel{fig:signal}{{7}{20}{Estimated signal tensors in the data analysis. (a) top edges associated with IQ scores in the brain connectivity data. The color indicates the estimated IQ effect size. (b) top authors and words for years 1996-1999 in the NIPS data. Authors and words are ranked by marginal averages based on $\hat \Theta $, where the marginal average is denoted in the parentheses. \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{20}{section.7}}
\newlabel{lastpage}{{7}{20}{Conclusion}{section.7}{}}
\bibstyle{chicago}
\bibdata{tensor_wang}
\bibcite{alon2016sign}{{1}{2016}{{Alon et~al.}}{{Alon, Moran, and Yehudayoff}}}
\bibcite{anandkumar2014tensor}{{2}{2014}{{Anandkumar et~al.}}{{Anandkumar, Ge, Hsu, Kakade, and Telgarsky}}}
\bibcite{anandkumar2017analyzing}{{3}{2017}{{Anandkumar et~al.}}{{Anandkumar, Ge, and Janzamin}}}
\bibcite{balabdaoui2019least}{{4}{2019}{{Balabdaoui et~al.}}{{Balabdaoui, Durot, and Jankowski}}}
\bibcite{bartlett2006convexity}{{5}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{cai2019nonconvex}{{6}{2019}{{Cai et~al.}}{{Cai, Li, Poor, and Chen}}}
\bibcite{chan2014consistent}{{7}{2014}{{Chan and Airoldi}}{{Chan and Airoldi}}}
\bibcite{chi2020provable}{{8}{2020}{{Chi et~al.}}{{Chi, Gaines, Sun, Zhou, and Yang}}}
\bibcite{cohn2013fast}{{9}{2013}{{Cohn and Umans}}{{Cohn and Umans}}}
\bibcite{de2000multilinear}{{10}{2000}{{De~Lathauwer et~al.}}{{De~Lathauwer, De~Moor, and Vandewalle}}}
\bibcite{de2003nondeterministic}{{11}{2003}{{De~Wolf}}{{De~Wolf}}}
\bibcite{fan2019online}{{12}{2019}{{Fan and Udell}}{{Fan and Udell}}}
\bibcite{ganti2017learning}{{13}{2017}{{Ganti et~al.}}{{Ganti, Rao, Balzano, Willett, and Nowak}}}
\bibcite{ganti2015matrix}{{14}{2015}{{Ganti et~al.}}{{Ganti, Balzano, and Willett}}}
\bibcite{ghadermarzy2018learning}{{15}{2018}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{ghadermarzy2019near}{{16}{2019}{{Ghadermarzy et~al.}}{{Ghadermarzy, Plan, and Yilmaz}}}
\bibcite{globerson2007euclidean}{{17}{2007}{{Globerson et~al.}}{{Globerson, Chechik, Pereira, and Tishby}}}
\bibcite{han2020optimal}{{18}{2020}{{Han et~al.}}{{Han, Willett, and Zhang}}}
\bibcite{hastie2009elements}{{19}{2009}{{Hastie et~al.}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{hillar2013most}{{20}{2013}{{Hillar and Lim}}{{Hillar and Lim}}}
\bibcite{hitchcock1927expression}{{21}{1927}{{Hitchcock}}{{Hitchcock}}}
\bibcite{hong2020generalized}{{22}{2020}{{Hong et~al.}}{{Hong, Kolda, and Duersch}}}
\@writefile{toc}{\contentsline {section}{References}{21}{section*.10}}
\bibcite{hore2016tensor}{{23}{2016}{{Hore et~al.}}{{Hore, Vi{\~n}uela, Buil, Knight, McCarthy, Small, and Marchini}}}
\bibcite{jain2014provable}{{24}{2014}{{Jain and Oh}}{{Jain and Oh}}}
\bibcite{kolda2009tensor}{{25}{2009}{{Kolda and Bader}}{{Kolda and Bader}}}
\bibcite{pmlr-v119-lee20i}{{26}{2020}{{Lee and Wang}}{{Lee and Wang}}}
\bibcite{li2009brain}{{27}{2009}{{Li et~al.}}{{Li, Liu, Li, Qin, Li, Yu, and Jiang}}}
\bibcite{montanari2018spectral}{{28}{2018}{{Montanari and Sun}}{{Montanari and Sun}}}
\bibcite{pmlr-v70-ongie17a}{{29}{2017}{{Ongie et~al.}}{{Ongie, Willett, Nowak, and Balzano}}}
\bibcite{robinson1988root}{{30}{1988}{{Robinson}}{{Robinson}}}
\bibcite{wang2017bayesian}{{31}{2017}{{Wang et~al.}}{{Wang, Durante, Jung, and Dunson}}}
\bibcite{wang2018learning}{{32}{2020}{{Wang and Li}}{{Wang and Li}}}
\bibcite{wang2019multiway}{{33}{2019}{{Wang and Zeng}}{{Wang and Zeng}}}
\bibcite{xu2018rates}{{34}{2018}{{Xu}}{{Xu}}}
\bibcite{yuan2016tensor}{{35}{2016}{{Yuan and Zhang}}{{Yuan and Zhang}}}
\bibcite{zhang2018tensor}{{36}{2018}{{Zhang and Xia}}{{Zhang and Xia}}}
