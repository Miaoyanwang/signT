\documentclass[10pt, mathserif]{beamer} %, mathserif
\renewcommand{\baselinestretch}{1.3}
\usepackage{bbm,tikz}
\usetheme{boxes}
\usecolortheme{rose}
\usepackage{multirow}
\setbeamertemplate{blocks}[rounded][shadow=false]
\setbeamertemplate{footline}[frame number]
\newtheorem{assumption}{Assumption}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{col}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem*{obs}{Observation}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{axiom}{Axiom}
\newtheorem{hyp}{Hypothesis}
\usepackage{multirow}
\theoremstyle{plain}
\setbeamertemplate{theorems}[numbered]
\usetikzlibrary{decorations.pathreplacing}
\newenvironment{sequation}{\small\begin{equation}}{\end{equation}}
\newenvironment{sequation*}{\small\begin{equation*}}{\end{equation*}}
\newenvironment{tequation}{\tiny\begin{equation}}{\end{equation}}
\newenvironment{tequation*}{\tiny\begin{equation*}}{\end{equation*}}
\usepackage{pstricks,egameps}


\def\trueB{\mB^{\text{true}}}
\def\newX{\mX_{\textup{new}}}
\def\newy{y_{\textup{new}}}
\def\sign{\textup{sgn}}

\def\srank{\textup{srank}}
\def\rank{\textup{rank}}
\def\caliP{\mathscr{P}_{\textup{sgn}}}
\def\risk{\textup{Risk}}
 \definecolor{darkgreen}{rgb}{0.0, 0.4, 0.1}

\newcommand*{\KeepStyleUnderBrace}[1]{%f
\mathop{%
\mathchoice
{\underbrace{\displaystyle#1}}%
{\underbrace{\textstyle#1}}%
{\underbrace{\scriptstyle#1}}%
{\underbrace{\scriptscriptstyle#1}}%
}\limits
}
\usepackage{pifont}
\usepackage{pgfpages}
\usepackage[ruled]{algorithm2e}

\setbeameroption{hide notes}
\setbeamertemplate{note page}{%
  \insertnote%
}
\usepackage{lmodern}
\setlength{\leftmargini}{0pt}
\setlength{\leftmarginii}{14pt}

\usepackage{dsfont}
\usepackage{wrapfig}
\usepackage{mathrsfs}

\def\caliP{\mathscr{P}_{\textup{sgn}}}


\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{appendixnumberbeamer}
\usepackage{appendix}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{setspace}

\usepackage{empheq}
\title[Higher-order tensors]{\\
\vspace{1.5cm}
Beyond the signs: Nonparametric tensor completion via sign series}
\author{Miaoyan Wang\\}
\date{}
\institute{
{\scriptsize
Department of Statistics, UW-Madison\\
\vspace{.4cm}
}
{\scriptsize
Joint work with Chanwoo Lee (3rd-year PhD student)\\
\vspace{.2cm}
\\
}
\hspace{8cm}\includegraphics[width=2cm]{Figures/image.png}
\\
}

    
\AtBeginSection[]

\usepackage{bm}
\input macros.tex

\begin{document}
\renewcommand{\raggedright}{\leftskip=0pt \rightskip=0pt plus 0cm}

\begin{frame}[plain]{}{}
\titlepage
\end{frame}


%\begin{frame}[plain]{Research in my group}

%{\bf Statistical machine learning:}
%\begin{itemize}
%\item Structured tensor decomposition, latent factor models
%\end{itemize}
%\bigskip
%\bigskip

%{\bf Genetics and genomics:}
%\begin{itemize}
%\item gene expression analyses, genetic association studies
%\end{itemize}
%\bigskip
%\bigskip

%{\bf Foundations of data science:}
%\begin{itemize}
%\item Statistical-computational tradeoff for big data analytics 
%\end{itemize}
%\end{frame}

\begin{frame}{A successful story: PCA of Europeans}

\centerline{\includegraphics[width=1\textwidth]{Figures/SNP.pdf}}
\end{frame}


\begin{frame}{Matrix methods are powerful, however...}

\centerline{ \includegraphics[width=.7\textwidth]{Figures/PCA.png}}

All Gaussian except points 17 and 39. 

left: matrix PCA;  right: principal components of kurtosis.

{\scriptsize Figure credit: Jason Morton and Lek-Heng Lim (2009/2015).}
\end{frame}


\begin{frame}{What is a tensor?}

\begin{itemize}

\item Tensors are generalizations of vectors and matrices:

\onslide \includegraphics[width=.9\textwidth]{Figures/defn.png}
\item An order-$k$ tensor $\tensor{A} = \entry{a_{i_1\dots\, i_k}}  \in \mathbb{R}^{d_1 \times \dots \times d_k}$ is a hypermatrix with dimensions $(d_1,\ldots,d_k)$ and entries $a_{i_1\dots i_k}\in\mathbb{R}$.
\item This talk will focus on tensor of order $3$ or greater, also known as \red{higher-order tensors}.
\end{itemize}
\end{frame}



\begin{frame}{Tensors in statistical modeling}
``Tensors are the new matrices'' that tie together a wide range of areas:
\bigskip
\begin{itemize}
\item Longitudinal social network data $\{\mY_t: t=1,...,n\}$
\item Spatio-temporal transcriptome data
\item Joint probability table of a set of variables $\mathbb{P}(X_1,X_2,X_3)$
\item Higher-order moments in topic models
\item Markov models for the phylogenetic tree
\end{itemize}
\bigskip
\hfill {\scriptsize W \& Song 2017, P. Hoff 2015, Montanari \& Richard 2014}\\
\hfill{\scriptsize  Anandkumar et al 2014, Mossel et al 2004, P. McCullagh 1987}
\end{frame}



\begin{frame}{Tensors in biomedical science}

\begin{itemize}
\item Many datasets come naturally in a multiway form.
\item Multi-tissue, multi-individual gene expression data could be organized as an order-3 tensor $\tY=\entry{y_{ijk}}\in \mathbb{R}^{n_I\times n_G\times n_T}$.
\item Multi-individual, multi-modal brain connectivity data could be organized as an order-4 tensor $\tY=\entry{y_{ijkl}}\in \mathbb{R}^{n_I\times n_b\times n_b \times n_m}$.
\end{itemize}
\bigskip
\centerline{\includegraphics[width=\textwidth]{Figures/example.pdf}}
\bigskip
\hfill {\scriptsize W., Fischer, et al, 2019, W. \& Li, 2020}\\
\end{frame}


\begin{frame}{Talk outline} 
 \begin{alertblock}{Prohibitive Computational Complexity}
Most higher-order tensor problems are NP-hard [Hillar \& Lim, 2013].
\end{alertblock}


\begin{columns}[T] \begin{column}{.4\textwidth}
\centering
\includegraphics[width=3cm]{Figures/NP.png}
\end{column}%
\hfill%
\begin{column}{.6\textwidth}
Fortunately, tensors sought in statistical and machine learning applications are often \color{red}specially structured:
\begin{itemize}
\item Low-rankness
\item Sparsity
\item Non-negativity
\item ...
\end{itemize}
\end{column}%
\end{columns}

\begin{block}{This talk is based on}
{\footnotesize {\color{blue}Beyond the Signs: Nonparametric Tensor Completion via Sign Series.} Lee and W. 2021
\url{https://arxiv.org/pdf/2102.00384.pdf}}
\end{block}
\end{frame}


%\begin{frame}[label=matrix]{Review: Matrix SVD for biclustering}
%\vspace{.2cm}

%\centerline{\includegraphics[width=.9\textwidth]{Figures/svd.pdf}}

%\begin{itemize}
%\item Columns of ${\bf U}$ describe patterns across samples
%\item Columns of ${\bf V}^T$ describe patterns across genes
%\end{itemize}

%\setstretch{0.6}{
%{\hfill \tiny Y Kluger et al, Genome Research (2003). 13(4): 703-71}\\
%{\hfill \tiny Data Science Specialization (COURSERA) by Brian Caffo and Jeff Leek}\\
%}

 

\begin{frame}{Setup: signal plus noise model}
     \begin{center}
    \includegraphics[width =\textwidth]{Figures/signalnoise.pdf}
    \end{center}
    We focus on two problems:
    \begin{itemize}
        \item Nonparametric tensor estimation: How to estimate the signal tensor $\Theta$ under {\color{red}a wide range of structures}?
        \item Tensor completion: How many {\color{red}observed tensor entries} do we need in order for consistent recovery?
    \end{itemize}
\end{frame}


\end{frame}


\begin{frame}{Various notions of low-rankness}
\begin{itemize}
\item Canonical polyadic (CP) low-rankness {\scriptsize[Kolda \& Bader '09]}: $\Theta=\sum_{r=1}^R\lambda_r\ms_r\otimes \mg_r\otimes \mt_r \in\mathbb{R}^{d_1\times d_2\times d_3}$.

\vspace{.3cm}
\centerline{\includegraphics[width=7cm]{Figures/CP_decomp2.pdf}}

\item Tucker low-rankness {\scriptsize[Lathauwer '00]}: $\Theta=\tC\times_1\mS\times_2 \mG\times_3\mT\in\mathbb{R}^{d_1\times d_2\times d_2}$.

\vspace{.5cm}
\centerline{\includegraphics[width=5cm]{Figures/HOSVD.pdf}}
\item Others: tensor train model {\scriptsize[Oseledet '11]}, tensor block model {\scriptsize[W. \& Zeng '19; Han, Luo, W. et al '20]}, etc.
\end{itemize}
\end{frame}


%\begin{frame}{Inadequacies of low-rank models}
 %\begin{itemize}
% \item Classical low-rank models (Jain \& Oh '14; Motanari \& Sun '18):
% \\[.5cm]

%   \begin{center}
%    \includegraphics[width =\textwidth]{Figures/classic.pdf}
%    \end{center}

  %   \item Two limitations of classical low-rank models:
 %    \begin{enumerate}
  %       \item Sensitivity to order-preserving transformation.
 %        \item Inadequacy for special structures.
  %   \end{enumerate}
% \end{itemize}
%\end{frame}

\begin{frame}{Inadequacies of  low-rank models}
 \begin{itemize}         
  \item Tensor rank is sensitive to order-preserving transformation.
\begin{columns}
\begin{column}{0.5\textwidth}
   \begin{center}
     \includegraphics[width=0.7\textwidth]{Figures/example1.pdf}
     \end{center}
\end{column}
\begin{column}{0.6\textwidth} 
\begin{align}
\Theta &= {1\over 1+\exp(-c\mathcal{Z})},\quad\text{where }\\ 
\tZ &= \bm a^{\otimes3}+ \bm b^{\otimes3}+ \bm c^{\otimes3}
\end{align}
$\Rightarrow$ $\Theta$ is high-rank but $\tZ$ is low-rank. 
\end{column}
\end{columns}
      
   \pause   
   
 \item Low-rank model fails to address several important structures.
 \begin{columns}
\begin{column}{0.55\textwidth}
   \begin{center}
     \includegraphics[width=0.7\textwidth]{Figures/example2.pdf}
     \end{center}
\end{column}
\begin{column}{0.6\textwidth}  
\includegraphics[width=.8\textwidth]{Figures/max.pdf}

$\Rightarrow$ Both $\Theta$ and $\tZ$ are full rank. 
\end{column}
\end{columns}
    
{\tiny \hfill The matrix analogy of $\Theta$ was studied in the context of graphon analysis by Chan \& Airoldi '14.}
 \end{itemize}
\end{frame}

\begin{frame}{Why sign matters?}

For a bounded tensor $\Theta\in[-1,1]^{d_1\times \cdots \times d_K}$, 
\[
\Theta \approx {1\over |\tH|}\sum_{\pi \in \tH} \sign(\Theta-\pi), \quad \text{where}\quad \tH=\left\{-1,\ldots,-{1\over H}, 0, {1\over H}, \ldots, 1\right\}. 
\]


    \begin{itemize}
        \item We do not observe $\Theta$; instead, we observe a noisy incomplete version $\tY$. 
           \item How to estimate the signal tensor $\Theta$ given data tensor $\tY$?
    \end{itemize}
         \begin{center}
    \includegraphics[width =\textwidth]{Figures/signalnoise.pdf}
    \end{center}
  
\end{frame}




\begin{frame}{Sign rank}
\begin{itemize}
    \item Key ideas: we use {\color{red}a local (nonparametric) notion of ``low-rankness''} to allow a broad family of signal tensors. 
        
    \item  Two tensors are sign equivalent, denoted $\Theta \simeq \Theta'$, if $\text{sgn}(\Theta) = \text{sgn}(\Theta')$.

    \item Define the {\color{red}sign rank} by
  \begin{align}
    \rlap{\textcolor{blue!10}{\rule[-\dp\strutbox]{235pt}{\baselineskip}}}\,\text{srank}(\Theta) = \min\{\text{rank}(\Theta')\colon \Theta'\simeq \Theta, \ \Theta'\in\mathbb{R}^{d_1\times\cdots\times d_K}\}.
\end{align}
    \pause
 \begin{center}
    \includegraphics[width =.9\textwidth]{Figures/signrank_new.pdf}
    \end{center}
\item For any strictly monotonic function $g\colon\mathbb{R}\rightarrow \mathbb{R}$ with $g(0) = 0$,
\[\rlap{\textcolor{blue!10}{\rule[-\dp\strutbox]{105pt}{\baselineskip}}}\,\text{srank}(\Theta)\leq \text{rank}(g(\Theta)).\]
\end{itemize}
\end{frame}

\begin{frame}{Sign representable tensors}
    \begin{block}{Sign representable tensors} 
A tensor $\Theta$ is called {\color{red}$r$-sign representable} if the tensor $(\Theta-\pi)$ has sign rank bounded by $r$ for all $\pi\in[-1,1].$ 
    \end{block}
    \begin{itemize}
   \item Most existing structured tensors belong to sign representable family:
    \begin{itemize}
      \item {\color{red}Low-rank} CP tensors, Tucker tensors, stochastic tensor block models.
       \item {\color{red}High-rank} tensors from GLM, single index models. 
        \item {\color{red}Tensors with repeating patterns}, e.g. earlier max/min hypergraphon model $\small \Theta(i_1,\ldots,i_K) = \log(1+\max(i_1,\ldots,i_K))$ is 2-sign representable.
    \end{itemize} 
    
       \item We propose the signal tensor family
       \begin{align}
         \rlap{\textcolor{blue!10}{\rule[-\dp\strutbox]{250pt}{\baselineskip}}}\,  \Theta\in\caliP(r):=\{\Theta\colon\text{srank}(\Theta-\pi)\leq r \text{ for all } \pi\in[-1,1]\}.
       \end{align}
      \end{itemize}
\end{frame}


\begin{frame}{Our solution: sign signal helps!}
 
     \begin{center}
     \includegraphics[width =\textwidth]{Figures/mainidea_step1.pdf}
        \end{center}
   \end{frame}

\begin{frame}{Our solution: sign signal helps!}
     \begin{center}
 \includegraphics[width =\textwidth]{Figures/mainidea.pdf}
         \end{center}
 \end{frame}

\begin{frame}{Sign representation}
\begin{itemize}
    \item We observe {\color{red}a noisy incomplete tensor} $\tY_\Omega$ with observed index set $\Omega\in[d_1]\times\cdots\times[d_K]$ under uniform sampling scheme. 
    \item We dichotomize the data into {\color{red}a series of sign tensors}:
\[
\{\sign(\tY_\Omega-\pi)\}_{\pi\in \tH}, \quad \text{where}\ \tH=\left\{-1,\ldots, -{1\over H}, 0, {1\over H}, \ldots, 1\right\}. 
\]
\end{itemize}
        
  \begin{center}
    \includegraphics[width =\textwidth]{Figures/representation.pdf}
    \end{center}
\end{frame}

\begin{frame}{Sign estimation via weighted classification}
    \begin{itemize}
    \item We estimate {\color{red}$\text{sgn}(\Theta-\pi)$} through {\color{red}$\text{sgn}(\tY_\Omega-\pi)$} via weighted classification.
    \item Objective function of weighted classification is
    \begin{align}
    L({\color{red}\tZ},\tY_\Omega-\pi) = \frac{1}{|\Omega|}\sum_{\pi\in\Omega}\underbrace{|\tY(\omega)-\pi|}_{\text{weight}}\times\underbrace{ |{\color{red}\text{sgn}(\tZ(\omega))}-\text{sgn}(\tY(\omega)-\pi)|}_{\text{classification loss}}
    \end{align}
    \end{itemize}
      \begin{center}
    \includegraphics[width =\textwidth]{Figures/weightedclassification.pdf}
    \end{center}
    
\end{frame}

\begin{frame}{Identification for sign tensor estimation}
   \begin{itemize}
    \item If $\Theta\in\caliP(r)$ is (locally) {\color{red}$\alpha$-smooth ($\alpha\neq 0$)} at $\pi$, we have {\color{red}a unique optimizer} such that\\[.1cm]
       $\hspace{2cm}\text{sgn}(\Theta-\pi) = \argmin\limits_{\tZ\colon \text{rank}(\tZ)\leq r} \mathbb{E}_{\tY_\Omega}L(\tZ,\tY_\Omega-\pi)$.\vspace*{.1cm}

    \item We obtain a series of optimizers $\{\hat \tZ_\pi\}_{\pi\in\tH}$ as
    \begin{align}
        \hat \tZ_\pi = \argmin_{\tZ\colon \text{rank}(\tZ)\leq r}L(\tZ,\tY_\Omega-\pi).
    \end{align}
    \end{itemize}
 \vspace{-.2cm} 
 
     \begin{center}
 \includegraphics[width = \textwidth]{Figures/weightedclassification.pdf}
 \end{center}
 
     
     \vspace{-.4cm}
 {\hfill \tiny  $^*$Uniqueness up to sign equivalence, meaning the optimizer $\Theta_{\text{opt}} \simeq \sign(\Theta-\pi)$.}
\end{frame}

\begin{frame}{Identification for sign tensor estimation}

%Let $G(\pi)=\mathbb{P}_{\omega\sim\Pi}(\Theta(\omega)\leq \Pi)$ denote the cumulative distribution function (CDF) of $\Theta(\omega)$ under $\omega\sim \Pi$. \\

We quantify smoothness of $\Theta$ using CDF $G(\pi)=\mathbb{P}_{\omega\sim\Pi}(\Theta(\omega)\leq \pi)$ and the induced pseudo density (i.e., histogram with bin size $\Delta s=d^{-K}$).
 \pause
 
Intuition: {\color{red}sign recovery} is harder at {\color{red}levels where point mass concentrates}. 
 \begin{center}
 \includegraphics[width = \textwidth]{Figures/cdf_new.pdf}
 \end{center}
 
  \vspace{-.2cm} 
 \begin{block}{$\alpha$-smoothness of signal tensor}
\begin{itemize}
\item Partition $[-1,1]=\tN \cup \tN^c$, where $\tN$ consists of levels whose pseudo density based on $\Delta s$-bin is uniformly bounded, and $\tN^c$ otherwise. 
\item $G(\pi)$ is (globally) {\color{red}$\alpha$-smooth} in that for {\color{red}all $\pi\in \tN$},
\vspace{-.2cm}
\begin{equation}\label{eq:smooth}
\sup_{\Delta s \leq t<\rho(\pi, \tN^c)}{G(\pi+{t})-G(\pi-{t})\over t^{\color{red}\alpha}} \leq c
\vspace{-.2cm}
\end{equation}
for two constants $\alpha>0,c >0$, where $\rho(\pi,\tN^c) = \min_{\pi'\in\tN^c}|\pi-\pi'|+\Delta s$. 
\end{itemize}
\end{block}


\end{frame}



\begin{frame}{Sign tensor estimation error}
\begin{itemize}
\item For two tensor $\Theta_1,\Theta_2$, define $\textup{MAE}(\Theta_1,\Theta_2) = \mathbb{E}_{\omega\in\Pi}|\Theta_1(\omega)-\Theta_2(\omega)|.$
\end{itemize}
    \begin{block}{Sign tensor estimation for $\pi\in\tN$ (Lee and W. 2021)}
    Suppose $\Theta\in\caliP(r)$ is $\alpha$-smooth. Let $d_{\max}=\max_{k\in[K]} d_k$. Then, with very high probability over $\tY_\Omega$, we have uniform error bound over $\pi\in\tN$,
\begin{align}
\textup{MAE}(\text{sgn}\hat \tZ_\pi, \text{sgn}(\Theta-\pi)) \lesssim  C(\pi)\left({d_{\max} r\over |\Omega|}\right)^{\color{red}\alpha\over \alpha+2}.
\end{align}
    \end{block}
    \begin{itemize}
    \item Sign estimation error shows a polynomial decay with $|\Omega|$. 
    \item Best rate is attained at $\alpha=\infty$ for stochastic tensor block models. 
    \end{itemize}
\end{frame}

\begin{frame}{From sign to signal estimation}
  
    \begin{itemize}
    \item Aggregation of sign tensors from weighted classification yields our signal tensor estimate:
    \begin{align}
        \hat\Theta = {1\over 2H+1}\sum_{\pi\in\tH}\text{sgn}\hat \tZ_\pi.
    \end{align}
    \vspace{-.5cm}
        \begin{center}
    \includegraphics[width = .9\textwidth]{Figures/aggregation.pdf}
    \end{center}
    \item Signal tensor estimation is robust to a few off-target classifications. 
    
        \begin{center}
    \includegraphics[width =.4\textwidth]{Figures/cdf_new_ave.pdf}
    \end{center}
    
    \end{itemize}

\end{frame}




\begin{frame}[label=bound]{Signal tensor estimation error}
    \begin{block}{Tensor estimation error (Lee and W. 2021)}
    Suppose $\Theta\in\caliP(r)$ is $\alpha$-smooth with bounded $|\tN^c|$. Then, with very high probability over $\tY_\Omega$, 
    \vspace{-0.3cm}
    \begin{equation}\label{eq:bound2}
\textup{MAE}(\hat \Theta, \Theta)\lesssim \KeepStyleUnderBrace{\left({d_{\max} r \over |\Omega|}\right)^{\alpha\over\alpha+2}}_{\text{Error inherited from sign estimation}}+\KeepStyleUnderBrace{{1\over \color{red}H}}_{\text{Bias}}+\KeepStyleUnderBrace{{{\color{red}H}d_{\max} r \over |\Omega|}}_{\text{Variance}}.
\end{equation}
In particular, setting $\scriptstyle H\asymp \left( |\Omega|\over d_{\max}r\right)^{1/2}$ yields the error bound
\begin{equation}\label{eq:real}
\textup{MAE}(\hat \Theta, \Theta)\lesssim \left(d_{\max}r \over |\Omega|\right)^{\color{red}{\alpha \over  \alpha+2}\wedge {1\over 2}}.
\end{equation}
    \end{block}
    \begin{itemize}
    \item See papers for general results that allow unbounded $|\tN^c|$ and sub-Gaussian noise. 

    \end{itemize}
    \vspace{-.5cm}
\hfill \hyperlink{extension}{\beamerbutton{extension}}
    
\end{frame}

\begin{frame}{Comparison to existing results}

  \begin{itemize}
        \item Sample requirement for tensor completion:
        \vspace{-.2cm}
         \[\text{MAE}(\hat\Theta,\Theta)\rightarrow 0, \text{ as } {|\Omega|\over {\color{red}d_{\max}r}}\rightarrow\infty.\]
    \end{itemize}


\begin{itemize}
\item Special case with full observation and equal dimension $d_1=\cdots=d_K=d$:
\end{itemize}

\resizebox{\columnwidth}{!}{
\begin{tabular}{ccl}
Model & Our rate (power of $d$) & Previous results\\
\hline
Tensor block model &$-(K-1)/2$& $\alpha = \infty$; minimax rate in W. \& Zeng '19\\
\hline
\multirow{2}{*}{Single index model}&\multirow{2}{*}{$-(K-1)/3$}& $\alpha =1$; conjecture on the optimality; matrix rate \\
&&  $d^{-1/3}$ improves $\tO(d^{-1/4})$ by Ganti et al. '18.\\
\hline
Rader type tensor&$-(K-2)/2$& close to parametric rate\\
\hline
 \multirow{2}{*}{$\alpha$-smooth $\caliP(r)$} & \multirow{2}{*}{$-(K-1)\min({\alpha\over\alpha+2}\wedge {1\over 2} )$}& \multirow{2}{*}{faster rate as $\alpha$ increases} \\
&&&
\hline
\end{tabular}
}
\end{frame}

\begin{frame}{Our contributions and related work}

\begin{block}{Our contributions}
\begin{itemize}
\item <1-> We develop a new model called {\color{red} sign representable tensors} to fill the gap between parametric (low-rank) and nonparametric (high-rank) tensors. 
\item <2-> Our tensor estimate is {\color{red}provably reducible} to a series of classifications. $\Rightarrow$ computationally efficient via a divide-and-conquer algorithm. 
\end{itemize}
\end{block}
\pause


\begin{itemize}
\item <3-> {\bf Low-rank tensor estimation} (Anandkumar et al.\ '14; Montanari \& Sun '18; Cai et al.\ '19) $\Rightarrow$ {\color{red}low-rank assumption is often violated} in practice. 
\item <4-> {\bf High-rank matrix estimation} was studied under nonlinear models (Ganti et al.\ '15), permutation rank (Shah et al.\ '18), and other shape constraints (Chatterjee et al. '19) $\Rightarrow$ tensors are more challenging because {\color{red}tensor rank may exceed dimension}. 
\end{itemize}


\end{frame}


\begin{frame}{Comparison of estimation error versus dimension}

    \begin{itemize}
\item We simulate signal tensors under a wide range of complexity.    

       \begin{center}
        \includegraphics[width = \textwidth]{Figures/simulation.pdf}
        \end{center}

\pause
    \item Our method ({\color{blue}NonparaT}) achieves the best performance, whereas the second best method is low-rank CP tensor ({\color{red} CPT}) for models 1-2, and matrix nonparametric method ({\color{darkgreen} NonParaM}) for models 3-4.
  \end{itemize}
    
          \begin{center}
    \includegraphics[width =\textwidth]{Figures/fig1-4v2.pdf}
    \end{center}
    
        
\end{frame}

\begin{frame}{Estimation error versus observation fraction}

    \begin{center}
        \includegraphics[width = \textwidth]{Figures/simulation.pdf}
        \end{center}
        
  \begin{itemize}
    \item Our method (NonparaT) achieves the best performance in completion. 
  \end{itemize}
    
    \begin{center}
  \includegraphics[width =\textwidth]{Figures/fig5-8v2.pdf}
  \end{center}
    
\end{frame}


\begin{frame}{Data application}

We apply our method to three datasets:
\begin{itemize}
\item The NIPS dataset (Globerson et al '07) consists of word occurrence counts in papers published from 1987 to 2003.

\item The human brain connectivity data (Want et al '17) consists of 68 brain regions for 114 individuals along with their IQ scores.

\item The 3-channel image data is from licensed google image file.


\end{itemize}
    \begin{center}
    \includegraphics[width =\textwidth]{Figures/ndataset.pdf}
    \end{center}
\end{frame}


\begin{frame}{Data application: NIPS}
    \begin{columns}
\begin{column}{0.21\textwidth}
   \begin{center}
     \includegraphics[width=\textwidth]{Figures/nipsdata.pdf}
     \end{center}
\end{column}
\begin{column}{0.7\textwidth} 
\begin{itemize}
    \item The NIPS dataset consists of word occurrence counts in papers published from 1987 to 2003.
    \item Data tensor $\tY\in\mathbb{R}^{100\times200\times17}.$ 
\end{itemize}
\end{column}
\end{columns}

\begin{columns}
\begin{column}{0.6\textwidth}
 \begin{itemize}
     \item We examine the estimated signal tensor $\hat\Theta.$
     \item Most frequent words are consistent with active topics in NIPS conference. 
     \item Strong heterogeneity among word occurrences across authors and years.
 \end{itemize}
\end{column}
\begin{column}{0.4\textwidth} 
   \begin{center}
     \includegraphics[width=\textwidth]{Figures/signal.pdf}
     \end{center}
\end{column}
\end{columns}
\end{frame}


\begin{frame}[label=result]{Data application: NIPS}
\begin{columns}
\begin{column}{0.5\textwidth} 
\scriptsize
\begin{itemize}
\item Top words: \emph{neural} (1.95), \emph{learning} (1.48), \emph{network} (1.21), \emph{training} (1.22), \emph{parameter (1.16)}.
\item Top authors: \emph{T. Sejnowski} (1.18), \emph{B. Sch\"{o}lkopf} (1.17), \emph{M. Jordan} (1.11), and \emph{G. Hinton} (1.06),
\item Top combinations: 
\\(\emph{\color{red}training}, \emph{\color{red}algorithm}) $\times$ (\emph{B. Sch\"{o}lkopf}, \emph{A. Smola}) $\times$ (1998, 1999), \\
(\emph{\color{blue}model}) $\times$ (\emph{M. Jordan}) $\times$ (1996).
\item Similar word patterns in 1998-1999:

(\emph{B. Sch\"{o}lkopf}, \emph{K-R M\"{o}ller}, \emph{A. Smola}, \emph{G. R\"{a}tsch}, \emph{J. Weston}) {\color{darkgreen}$\Rightarrow$ Co-authors}. 
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth} 
\includegraphics[width=1\textwidth]{Figures/signal.pdf}
\end{column}
\end{columns}

\vspace{1cm}
{\hfill \scriptsize Numbers in parentheses denote marginal averages based on $\hat \Theta$\\
\hfill \hyperlink{NIPS}{\beamerbutton{details}}
}
\end{frame}


\begin{frame}{Data application: Brain connectivity}
\begin{columns}
\begin{column}{0.21\textwidth}
   \begin{center}
     \includegraphics[width=\textwidth]{Figures/braindata.pdf}
     \end{center}
\end{column}
\begin{column}{0.7\textwidth} 
\begin{itemize}
    \item The human brain connectivity data consists of 68 brain regions for 114 individuals along with their IQ scores.
    \item  Data tensor $\tY\in\{0,1\}^{68\times68\times114}.$
\end{itemize}
\end{column}
\end{columns}

\begin{columns}
\begin{column}{0.6\textwidth}
 \begin{itemize}
     \item We examine the estimated signal tensor $\hat\Theta.$
     \item Top 10 brain edges based on regression analysis show inter-hemisphere connections.
 \end{itemize}
\end{column}
\begin{column}{0.4\textwidth} 
   \begin{center}
     \includegraphics[width=\textwidth]{Figures/brainIQ.pdf}
     \end{center}
\end{column}
\end{columns}

\end{frame}


\begin{frame}{Data application: Brain connectivity + NIPS}
\begin{itemize}
\item Our method achieves better test performance than low-rank methods. 
\end{itemize}
      \begin{center}
     \begin{table}
    \includegraphics[width =\textwidth]{Figures/cvtable.pdf}
    \caption{\scriptsize MAE comparison in the brain data and NIPS data based on 5-folded cross-validations. Standard errors are reported in parenthesis.}
    \end{table}
    \end{center}
\end{frame}

\begin{frame}{Data application: Image}
 \begin{columns}
\begin{column}{0.20\textwidth}
   \begin{center}
     \includegraphics[width=\textwidth]{Figures/imgdata.pdf}
     \end{center}
\end{column}
\begin{column}{0.7\textwidth} 
\begin{itemize}
    \item The original data is from licensed google image file.
    \item Data tensor $\tY\in[0,1]^{217\times217\times3}.$ 
    \item We assess completion performance by sampling 50\% entries in the original image tensor.
\end{itemize}
\end{column}
\end{columns}


      \begin{center}
    \includegraphics[width =\textwidth]{Figures/imageresult.pdf}
    \end{center}
\end{frame}


\begin{frame}{Summary}

Tensor analysis provides a rich source of
\begin{itemize}
\item fundamental problems in data science.
\item new tools for long-standing questions.
\item potentials for new applications.
\end{itemize}

\begin{block}{}
Our general strategy is to develop efficient statistical methods for analyzing a broad range of {\color{red}specially-structured tensors} that are useful in practice. 
\end{block}

References:
\begin{itemize}
\item {\color{blue}Beyond the signs: nonparametric tensor completion from sign series. \url{https://arxiv.org/pdf/2102.00384.pdf}}
\end{itemize}
\vspace{.2cm}
Acknowledgment: {\small NSF DMS-1915978, DMS-2023239, and Grant from Wisconsin Alumni Research Foundation.}
\end{frame}



\appendix
\begin{frame}{Appendix}
    \includegraphics[width = \textwidth]{Figures/algorithm.pdf}
\end{frame}


\begin{frame}[label=NIPS]{Paper titles in NIPS dataset}
\tiny
1998:

\begin{itemize}
\item {\color{red} Kernel} PCA and De-Noising in Feature Spaces. Sebastian Mika, {\bf B. Sch\"{o}lkopf}, {\bf A. Smola}, {\bf K. M\"{u}ller}, M.\ Scholz, {\bf G.\ R\"{a}tsch}
\item  Shrinking the Tube: A New {\color{red} Support Vector Regression Algorithm}. {\bf B. Sch\"{o}lkopf}, P. Bartlett, {\bf A. Smola}, R. C. Williamson
\item Semiparametric {\color{red} Support Vector} and Linear Programming Machines. {\bf A. Smola}, T-T. Frieb, {\bf B. Sch\"{o}lkopf}
\item Regularizing AdaBoost. {\bf G. R\"{a}tsch}, T. Onoda, {\bf K. M\"{u}ller}
\end{itemize}

\bigskip

1999:
\begin{itemize}
\item v-Arc: Ensemble Learning in the Presence of Outliers. {\bf G. R\"{a}tsch}, {\bf B. Sch\"{o}lkopf}, {\bf A. Smola}, {\bf K. M\"{u}ller}, T. Onoda, S. Mika
\item Invariant Feature Extraction and Classification in Kernel Spaces. S. Mika, {\bf G. R\"{a}tsch}, {\bf J. Weston}, {\bf B. Sch\"{o}lkopf}, {\bf A. Smola}, {\bf K M\"{u}ller}
\item {\color{red} Support Vector Method} for Novelty Detection. {\bf B. Sch\"{o}lkopf}, R. C. Williamson, {\bf A. Smola}, J. S. Taylor, J. Platt.
\end{itemize}

\bigskip
1996:
\begin{itemize}
\item A Variational Principle for {\color{blue}Model}-based Morphing. L. Saul, {\bf M. Jordan}
\item Hidden Markov Decision Trees. {\bf M. Jordan}, Z. Ghahramani, L. Saul
\item Triangulation by Continuous Embedding. M. Meila, {\bf M. Jordan}
\item Recursive Algorithms for Approximating Probabilities in Graphical {\color{blue}Models}. T. Jaakkola, {\bf M. Jordan}
\end{itemize}


{\hfill \hyperlink{result}{\beamerbutton{Back to NIPS analysis}}}
\end{frame}

%\begin{frame}
%\frametitle{References}
%\tiny
%\begin{itemize}
%\item Oseledets, Ivan V. Tensor-train decomposition. SIAM Journal on Scientific Computing 33.5 (2011): 2295-2317.
%\item M. Wang and Y. Zeng. Multiway clustering via tensor block models. Advances in Neural Information Processing Systems 32 (NeurIPS), 715-725, 2019
%\item R. Han, Y. Luo, M. Wang, and A. R. Zhang. Exact clustering in tensor block model: Statistical optimality and computational limit. Under review, 2020.
%\end{itemize}



%\bibliography{tensor_wang}
%\end{frame}

\begin{frame}[label=extension]{Extension to sub-Gaussian noise}
Consider the signal plus noise model
\[
\tY=\Theta+\tE,
\]
where $\Theta\in\caliP(r)$ is an $\alpha$-smooth tensor with $\mnormSize{}{\Theta}\leq \beta$, and $\tE$ consists of independent sub-Gaussian noise with variance proxy $\sigma^2$. 

    \begin{block}{Nonparametric tensor estimation with sub-Gaussian noise}
    With high probability over $\tY_{\Omega}$, we have
\begin{equation}\label{eq:real}
\textup{MAE}(\hat \Theta, \Theta)\lesssim \left((\sigma^2 +\beta^2) rd_{\max} \log d_{\max}  \over |\Omega|\right)^{\color{red}{\alpha \over \alpha+2} \vee {1\over 2}}.
\end{equation}
    \end{block}


{\hfill \hyperlink{bound}{\beamerbutton{Back to bounded result}}}

\end{frame}

\begin{frame}

\begin{block}{Tensor estimation error (Lee and W. 2021)} Consider a signal plus noise model with $\alpha$-smooth signal $\Theta \in \caliP(r)$. Let $\hat \Theta$ be the our nonparametric estimate, and $|\tN^c|$ the covering number of $\tN^c$ with $\Delta s$-bin's, i.e, $|\tN^c| = \text{Leb}(\tN^c)/\Delta s$. Write $t_d={d_{\max}r /|\Omega|}$. With very high probability over $\tY_\Omega$, 

\begin{equation}\label{eq:bound2}
\textup{MAE}(\hat \Theta, \Theta)\lesssim \KeepStyleUnderBrace{\left(t_d\right)^{\alpha/(\alpha+2)}}_{\text{Error inherited from sign estimation}}+\KeepStyleUnderBrace{{1+{\color{red}|\tN^c|}\over H}}_{\text{Bias}}+\KeepStyleUnderBrace{Ht_d}_{\text{Bariance}}.
\end{equation}
In particular, setting $\scriptstyle H\asymp \left( (1+|\tN^c|)/ t_d \right)^{1/2}$ yields the error bound
\begin{equation}\label{eq:real}
\textup{MAE}(\hat \Theta, \Theta)\lesssim \max\left(t_d^{{2\alpha/ (\alpha+2)} },\ t_d(1+|\tN^c|)\right)^{1/2}.
\end{equation}
\end{block}
{\hfill \hyperlink{bound}{\beamerbutton{Back to bounded result}}}
\end{frame}

\end{document}
