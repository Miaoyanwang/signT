\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{anandkumar2014tensor}
\citation{wang2017bayesian}
\citation{hore2016tensor}
\citation{jain2014provable,montanari2018spectral}
\citation{hitchcock1927expression}
\citation{de2000multilinear}
\citation{wang2019multiway}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\newlabel{eq:modelintro}{{1}{1}{Introduction}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Inadequacies of low-rank models}{1}{subsection.1.1}}
\MT@newlabel{eq:model}
\citation{ghadermarzy2018learning}
\citation{hore2016tensor}
\citation{chan2014consistent}
\citation{anandkumar2014tensor,montanari2018spectral,cai2019nonconvex}
\MT@newlabel{eq:modelintro}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Numerical rank of $\Theta $ versus $c$ in the first example. (b) Top $d=30$ tensor singular values in the second example. \relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:example}{{1}{2}{(a) Numerical rank of $\Theta $ versus $c$ in the first example. (b) Top $d=30$ tensor singular values in the second example. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Our contributions}{2}{subsection.1.2}}
\citation{ghadermarzy2018learning,wang2018learning,han2020optimal}
\citation{ganti2015matrix}
\citation{pmlr-v70-ongie17a,fan2019online}
\citation{anandkumar2017analyzing}
\citation{hitchcock1927expression}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of our method. For visualization purpose, we plot an order-2 tensor (a.k.a.\ matrix); similar procedure applies to higher-order tensors. (a): a noisy and incomplete tensor input. (b) and (c): main steps of estimating sign tensor series $\textup  {sgn}(\Theta -\pi )$ for $\pi \in \{-1,\ldots  ,-{1\over H},0,{1\over H},\ldots  ,1\}$. (d) estimated signal $\mathaccentV {hat}05E\Theta $. The depicted signal is a full-rank matrix based on Example\nobreakspace  {}\ref  {eq:example} in Section\nobreakspace  {}\ref  {sec:representation}.\relax }}{3}{figure.caption.2}}
\newlabel{fig:demo}{{2}{3}{Illustration of our method. For visualization purpose, we plot an order-2 tensor (a.k.a.\ matrix); similar procedure applies to higher-order tensors. (a): a noisy and incomplete tensor input. (b) and (c): main steps of estimating sign tensor series $\sign (\Theta -\pi )$ for $\pi \in \{-1,\ldots ,-{1\over H},0,{1\over H},\ldots ,1\}$. (d) estimated signal $\hat \Theta $. The depicted signal is a full-rank matrix based on Example~\ref {eq:example} in Section~\ref {sec:representation}.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Notation}{3}{subsection.1.3}}
\newlabel{eq:CP}{{2}{4}{Notation}{equation.1.2}{}}
\MT@newlabel{eq:CP}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model and proposal overview}{4}{section.2}}
\newlabel{sec:overview}{{2}{4}{Model and proposal overview}{section.2}{}}
\newlabel{eq:model}{{3}{4}{Model and proposal overview}{equation.2.3}{}}
\MT@newlabel{eq:model}
\citation{cohn2013fast}
\citation{alon2016sign}
\citation{de2003nondeterministic}
\citation{hillar2013most}
\citation{alon2016sign}
\@writefile{toc}{\contentsline {section}{\numberline {3}Statistical properties of sign representable tensors}{5}{section.3}}
\newlabel{sec:representation}{{3}{5}{Statistical properties of sign representable tensors}{section.3}{}}
\MT@newlabel{eq:model}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sign-rank and sign tensor series}{5}{subsection.3.1}}
\newlabel{sec:sign-rank}{{3.1}{5}{Sign-rank and sign tensor series}{subsection.3.1}{}}
\newlabel{cor:monotonic}{{1}{5}{Upper bounds of the sign-rank}{prop.1}{}}
\newlabel{prop:extention}{{2}{5}{Broadness}{prop.2}{}}
\newlabel{cor:broadness}{{2}{5}{Broadness}{prop.2}{}}
\citation{kolda2009tensor}
\citation{wang2019multiway,chi2020provable}
\citation{wang2018learning}
\citation{hong2020generalized}
\citation{robinson1988root}
\citation{balabdaoui2019least,ganti2017learning}
\citation{chan2014consistent,xu2018rates}
\MT@newlabel{eq:model}
\newlabel{eq:example}{{5}{6}{Min/Max hypergraphon}{example.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Statistical characterization of sign tensors via weighted classification}{7}{subsection.3.2}}
\newlabel{sec:identifiability}{{3.2}{7}{Statistical characterization of sign tensors via weighted classification}{subsection.3.2}{}}
\newlabel{eq:sample}{{4}{7}{Statistical characterization of sign tensors via weighted classification}{equation.3.4}{}}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:sample}
\newlabel{eq:population}{{5}{7}{Statistical characterization of sign tensors via weighted classification}{equation.3.5}{}}
\MT@newlabel{eq:model}
\newlabel{prop:global}{{3}{7}{Global optimum of weighted risk}{prop.3}{}}
\MT@newlabel{eq:model}
\newlabel{eq:optimal}{{3}{7}{Global optimum of weighted risk}{prop.3}{}}
\newlabel{ass:margin}{{1}{8}{$\alpha $-smoothness}{assumption.1}{}}
\newlabel{eq:smooth}{{6}{8}{$\alpha $-smoothness}{equation.3.6}{}}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\newlabel{thm:population}{{1}{8}{Identifiability}{thm.1}{}}
\MT@newlabel{eq:population}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:smooth}
\@writefile{toc}{\contentsline {section}{\numberline {4}Nonparametric tensor completion via sign series}{8}{section.4}}
\newlabel{sec:estimation}{{4}{8}{Nonparametric tensor completion via sign series}{section.4}{}}
\citation{hastie2009elements}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Estimation error and sample complexity}{9}{subsection.4.1}}
\MT@newlabel{eq:model}
\newlabel{eq:est}{{7}{9}{Estimation error and sample complexity}{equation.4.7}{}}
\newlabel{eq:estimate}{{8}{9}{Estimation error and sample complexity}{equation.4.8}{}}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:est}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:est}
\newlabel{thm:classification}{{2}{9}{Sign tensor estimation}{thm.2}{}}
\MT@newlabel{eq:estimate}
\newlabel{eq:bound}{{9}{9}{Sign tensor estimation}{equation.4.9}{}}
\newlabel{thm:estimation}{{3}{9}{Tensor estimation error}{thm.3}{}}
\MT@newlabel{eq:est}
\newlabel{eq:bound2}{{10}{9}{Tensor estimation error}{equation.4.10}{}}
\newlabel{eq:real}{{11}{9}{Tensor estimation error}{equation.4.11}{}}
\MT@newlabel{eq:bound2}
\citation{wang2019multiway}
\citation{zhang2018tensor,wang2018learning}
\citation{ganti2015matrix}
\citation{yuan2016tensor,ghadermarzy2019near,pmlr-v119-lee20i}
\MT@newlabel{eq:real}
\MT@newlabel{eq:bound}
\citation{ghadermarzy2018learning,wang2018learning,hong2020generalized}
\citation{bartlett2006convexity}
\citation{ghadermarzy2018learning}
\citation{wang2018learning,han2020optimal}
\citation{anandkumar2014tensor,hong2020generalized}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Numerical implementation}{11}{subsection.4.2}}
\MT@newlabel{eq:est}
\MT@newlabel{eq:estimate}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Nonparametric tensor completion\relax }}{11}{algorithm.1}}
\newlabel{alg:tensorT}{{1}{11}{Nonparametric tensor completion\relax }{algorithm.1}{}}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:CP}
\MT@newlabel{eq:est}
\citation{wang2017bayesian}
\citation{globerson2007euclidean}
\@writefile{toc}{\contentsline {section}{\numberline {5}Simulations}{12}{section.5}}
\newlabel{sec:simulation}{{5}{12}{Simulations}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Simulation models used for comparison. We use $\bm  {M}_k\in \{0,1\}^{d\times 3}$ to denote membership matrices, $\mathcal  {C}\in \mathbb  {R}^{3\times 3\times 3}$ the block means, $\bm  {a}={1\over d}(1,2,\ldots  ,d)^T \in \mathbb  {R}^d$, $\mathcal  {Z}_{\qopname  \relax m{max}}$ and $\mathcal  {Z}_{\qopname  \relax m{min}}$ are order-3 tensors with entries ${1\over d}\qopname  \relax m{max}(i,j,k)$ and ${1\over d}\qopname  \relax m{min}(i,j,k)$, respectively.\relax }}{12}{table.caption.3}}
\newlabel{tab:simulation}{{1}{12}{Simulation models used for comparison. We use $\mM _k\in \{0,1\}^{d\times 3}$ to denote membership matrices, $\tC \in \mathbb {R}^{3\times 3\times 3}$ the block means, $\ma ={1\over d}(1,2,\ldots ,d)^T \in \mathbb {R}^d$, $\tZ _{\max }$ and $\tZ _{\min }$ are order-3 tensors with entries ${1\over d}\max (i,j,k)$ and ${1\over d}\min (i,j,k)$, respectively.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Data applications}{12}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Estimation error versus tensor dimension. Panels (a)-(d) correspond to simulation models 1-4 in Table\nobreakspace  {}\ref  {tab:simulation}.\relax }}{13}{figure.caption.4}}
\newlabel{fig:compare1}{{3}{13}{Estimation error versus tensor dimension. Panels (a)-(d) correspond to simulation models 1-4 in Table~\ref {tab:simulation}.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Completion error versus observation fraction. Panels (a)-(d) correspond to simulation models 1-4 in Table\nobreakspace  {}\ref  {tab:simulation}. \relax }}{13}{figure.caption.5}}
\newlabel{fig:compare2}{{4}{13}{Completion error versus observation fraction. Panels (a)-(d) correspond to simulation models 1-4 in Table~\ref {tab:simulation}. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Brain connectivity analysis}{13}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Estimation error versus rank under different missing rate. Panels (a)-(d) correspond to missing rate 20\%, 33\%, 50\%, and 67\%, respectively. Error bar represents the standard error over 5-fold cross-validations.\relax }}{13}{figure.caption.6}}
\newlabel{fig:braincv}{{5}{13}{Estimation error versus rank under different missing rate. Panels (a)-(d) correspond to missing rate 20\%, 33\%, 50\%, and 67\%, respectively. Error bar represents the standard error over 5-fold cross-validations.\relax }{figure.caption.6}{}}
\citation{li2009brain,wang2017bayesian}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces MAE comparison in the brain data and NIPS data analysis. Reported MAEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Bold numbers indicate the minimal MAE among three methods. For low-rank CPT, we use R function {\tt  rTensor} with default hyperparameters, and for our method, we set $H=20$.\relax }}{14}{table.caption.7}}
\newlabel{tab:data}{{2}{14}{MAE comparison in the brain data and NIPS data analysis. Reported MAEs are averaged over five runs of cross-validation, with 20\% entries for testing and 80\% for training, with standard errors in parentheses. Bold numbers indicate the minimal MAE among three methods. For low-rank CPT, we use R function {\tt rTensor} with default hyperparameters, and for our method, we set $H=20$.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}NIPS data analysis}{14}{subsection.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Estimated signal tensors in the data analysis. (a) top edges associated with IQ scores in the brain connectivity data. The color indicates the estimated IQ effect size. (b) top authors and words for years 1996-1999 in the NIPS data. Authors and words are ranked by marginal averages based on $\mathaccentV {hat}05E\Theta $, where the marginal average is denoted in the parentheses. \relax }}{15}{figure.caption.8}}
\newlabel{fig:signal}{{6}{15}{Estimated signal tensors in the data analysis. (a) top edges associated with IQ scores in the brain connectivity data. The color indicates the estimated IQ effect size. (b) top authors and words for years 1996-1999 in the NIPS data. Authors and words are ranked by marginal averages based on $\hat \Theta $, where the marginal average is denoted in the parentheses. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Additional results and proofs}{15}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Sensitivity of tensor rank to monotonic transformations}{15}{subsection.7.1}}
\newlabel{sec:additional}{{7.1}{15}{Sensitivity of tensor rank to monotonic transformations}{subsection.7.1}{}}
\newlabel{eq:numeric}{{7.1}{16}{Sensitivity of tensor rank to monotonic transformations}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Tensor rank and sign-rank}{16}{subsection.7.2}}
\newlabel{sec:high-rank}{{7.2}{16}{Tensor rank and sign-rank}{subsection.7.2}{}}
\newlabel{example:max}{{6}{16}{Max hypergraphon}{example.6}{}}
\newlabel{eq:matrix}{{12}{16}{Tensor rank and sign-rank}{equation.7.12}{}}
\MT@newlabel{eq:matrix}
\newlabel{eq:entrywise}{{13}{16}{Tensor rank and sign-rank}{equation.7.13}{}}
\MT@newlabel{eq:entrywise}
\newlabel{eq:max}{{14}{17}{Min/Max hypergraphon}{equation.7.14}{}}
\MT@newlabel{eq:max}
\newlabel{eq:support}{{15}{17}{Tensor rank and sign-rank}{equation.7.15}{}}
\MT@newlabel{eq:support}
\newlabel{eq:indicator}{{16}{17}{Tensor rank and sign-rank}{equation.7.16}{}}
\MT@newlabel{eq:indicator}
\newlabel{eq:sum}{{17}{18}{Tensor rank and sign-rank}{equation.7.17}{}}
\MT@newlabel{eq:sum}
\newlabel{example:banded}{{7}{18}{Stacked banded matrices}{example.7}{}}
\newlabel{eq:A}{{18}{18}{Tensor rank and sign-rank}{equation.7.18}{}}
\newlabel{eq:decrease}{{19}{19}{Tensor rank and sign-rank}{equation.7.19}{}}
\MT@newlabel{eq:decrease}
\MT@newlabel{eq:A}
\MT@newlabel{eq:A}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Proofs}{19}{subsection.7.3}}
\newlabel{sec:proofs}{{7.3}{19}{Proofs}{subsection.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Proofs of Propositions\nobreakspace  {}\ref  {cor:monotonic}-\ref  {prop:global}}{19}{subsubsection.7.3.1}}
\newlabel{eq:risk}{{20}{19}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global}}{equation.7.20}{}}
\newlabel{eq:I}{{21}{20}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global}}{equation.7.21}{}}
\MT@newlabel{eq:I}
\MT@newlabel{eq:I}
\MT@newlabel{eq:risk}
\newlabel{eq:minimum}{{22}{20}{Proofs of Propositions~\ref {cor:monotonic}-\ref {prop:global}}{equation.7.22}{}}
\MT@newlabel{eq:minimum}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Proof of Theorem\nobreakspace  {}\ref  {thm:population}}{20}{subsubsection.7.3.2}}
\MT@newlabel{eq:minimum}
\newlabel{eq:population2}{{23}{20}{Proof of Theorem~\ref {thm:population}}{equation.7.23}{}}
\newlabel{eq:ass}{{24}{20}{Proof of Theorem~\ref {thm:population}}{equation.7.24}{}}
\MT@newlabel{eq:population2}
\newlabel{eq:1}{{25}{20}{Proof of Theorem~\ref {thm:population}}{equation.7.25}{}}
\MT@newlabel{eq:ass}
\MT@newlabel{eq:1}
\citation{shen1994convergence}
\MT@newlabel{eq:1}
\newlabel{eq:MAE}{{7.3.2}{21}{Proof of Theorem~\ref {thm:population}}{equation.7.25}{}}
\MT@newlabel{eq:1}
\newlabel{eq:2}{{26}{21}{Proof of Theorem~\ref {thm:population}}{equation.7.26}{}}
\MT@newlabel{eq:2}
\newlabel{eq:rmk}{{2}{21}{}{rmk.2}{}}
\newlabel{eq:remark}{{27}{21}{}{equation.7.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Proof of Theorem\nobreakspace  {}\ref  {thm:classification}}{21}{subsubsection.7.3.3}}
\newlabel{lem:variance}{{1}{21}{Variance-to-mean relationship}{lem.1}{}}
\newlabel{eq:sample2}{{28}{21}{Variance-to-mean relationship}{equation.7.28}{}}
\newlabel{eq:variance}{{29}{21}{Variance-to-mean relationship}{equation.7.29}{}}
\citation{wang2008probability}
\citation{wang2008probability}
\citation{wang2008probability}
\newlabel{eq:mae}{{30}{22}{Proof of Theorem~\ref {thm:classification}}{equation.7.30}{}}
\MT@newlabel{eq:variance}
\MT@newlabel{eq:mae}
\MT@newlabel{eq:sample2}
\newlabel{eq:empirical}{{31}{22}{Proof of Theorem~\ref {thm:classification}}{equation.7.31}{}}
\MT@newlabel{eq:empirical}
\newlabel{eq:second}{{32}{22}{Proof of Theorem~\ref {thm:classification}}{equation.7.32}{}}
\MT@newlabel{eq:empirical}
\MT@newlabel{eq:second}
\newlabel{eq:rate}{{33}{22}{Proof of Theorem~\ref {thm:classification}}{equation.7.33}{}}
\newlabel{eq:equation}{{34}{22}{Proof of Theorem~\ref {thm:classification}}{equation.7.34}{}}
\citation{kosorok2007introduction}
\MT@newlabel{eq:equation}
\MT@newlabel{eq:equation}
\MT@newlabel{eq:equation}
\newlabel{eq:tn}{{7.3.3}{23}{Proof of Theorem~\ref {thm:classification}}{equation.7.34}{}}
\MT@newlabel{eq:rate}
\MT@newlabel{eq:remark}
\newlabel{eq:final}{{35}{23}{Proof of Theorem~\ref {thm:classification}}{equation.7.35}{}}
\MT@newlabel{eq:final}
\newlabel{pro:inftynorm}{{3}{23}{Bracketing number}{defn.3}{}}
\newlabel{lem:metric}{{2}{23}{Bracketing complexity of low-rank tensors}{lem.2}{}}
\newlabel{eq:specification}{{2}{23}{Bracketing complexity of low-rank tensors}{lem.2}{}}
\newlabel{eq:L}{{36}{23}{Bracketing complexity of low-rank tensors}{equation.7.36}{}}
\citation{mu2014square}
\MT@newlabel{eq:L}
\newlabel{eq:complexity}{{37}{24}{Proof of Theorem~\ref {thm:classification}}{equation.7.37}{}}
\MT@newlabel{eq:complexity}
\newlabel{eq:g}{{38}{24}{Proof of Theorem~\ref {thm:classification}}{equation.7.38}{}}
\MT@newlabel{eq:L}
\MT@newlabel{eq:g}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Proof of Theorem\nobreakspace  {}\ref  {thm:estimation}}{24}{subsubsection.7.3.4}}
\newlabel{eq:pfmain3}{{39}{25}{Proof of Theorem~\ref {thm:estimation}}{equation.7.39}{}}
\MT@newlabel{eq:pfmain3}
\newlabel{eq:total}{{40}{25}{Proof of Theorem~\ref {thm:estimation}}{equation.7.40}{}}
\MT@newlabel{eq:total}
\newlabel{eq:twobounds}{{41}{25}{Proof of Theorem~\ref {thm:estimation}}{equation.7.41}{}}
\MT@newlabel{eq:twobounds}
\MT@newlabel{eq:total}
\MT@newlabel{eq:total}
\MT@newlabel{eq:pfmain3}
\newlabel{lem:H}{{3}{25}{}{lem.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Extension of Theorems\nobreakspace  {}\ref  {thm:classification} and\nobreakspace  {}\ref  {thm:estimation} to unbounded observation with sub-Gaussian noise}{26}{subsection.7.4}}
\newlabel{sec:subGaussian}{{7.4}{26}{Extension of Theorems~\ref {thm:classification} and~\ref {thm:estimation} to unbounded observation with sub-Gaussian noise}{subsection.7.4}{}}
\newlabel{assm:subg}{{2}{26}{Sub-Gaussian noise}{assumption.2}{}}
\newlabel{thm:unbddno1}{{4}{26}{Sign tensor estimation under sub-Gaussian noise}{thm.4}{}}
\newlabel{eq:variance2}{{42}{27}{Extension of Theorems~\ref {thm:classification} and~\ref {thm:estimation} to unbounded observation with sub-Gaussian noise}{equation.7.42}{}}
\MT@newlabel{eq:variance2}
\newlabel{eq:vartomean}{{43}{27}{Extension of Theorems~\ref {thm:classification} and~\ref {thm:estimation} to unbounded observation with sub-Gaussian noise}{equation.7.43}{}}
\MT@newlabel{eq:vartomean}
\newlabel{eq:empriskbd}{{44}{27}{Extension of Theorems~\ref {thm:classification} and~\ref {thm:estimation} to unbounded observation with sub-Gaussian noise}{equation.7.44}{}}
\newlabel{eq:subgbd}{{45}{27}{Extension of Theorems~\ref {thm:classification} and~\ref {thm:estimation} to unbounded observation with sub-Gaussian noise}{equation.7.45}{}}
\MT@newlabel{eq:empriskbd}
\MT@newlabel{eq:subgbd}
\newlabel{eq:riskunbd}{{46}{27}{Extension of Theorems~\ref {thm:classification} and~\ref {thm:estimation} to unbounded observation with sub-Gaussian noise}{equation.7.46}{}}
\MT@newlabel{eq:riskunbd}
\MT@newlabel{eq:final}
\newlabel{eq:bound2}{{47}{27}{Tensor estimation error under sub-Gaussian noise}{equation.7.47}{}}
\newlabel{eq:real}{{48}{27}{Tensor estimation error under sub-Gaussian noise}{equation.7.48}{}}
\newlabel{lem:subg}{{4}{27}{sub-Gaussian maximum}{lem.4}{}}
\bibdata{tensor_wang.bib}
\bibcite{alon2016sign}{{1}{2016}{{Alon et~al.}}{{}}}
\bibcite{anandkumar2014tensor}{{2}{2014}{{Anandkumar et~al.}}{{}}}
\bibcite{anandkumar2017analyzing}{{3}{2017}{{Anandkumar et~al.}}{{}}}
\bibcite{balabdaoui2019least}{{4}{2019}{{Balabdaoui et~al.}}{{}}}
\bibcite{bartlett2006convexity}{{5}{2006}{{Bartlett et~al.}}{{}}}
\bibcite{cai2019nonconvex}{{6}{2019}{{Cai et~al.}}{{}}}
\bibcite{chan2014consistent}{{7}{2014}{{Chan and Airoldi}}{{}}}
\bibcite{chi2020provable}{{8}{2020}{{Chi et~al.}}{{}}}
\bibcite{cohn2013fast}{{9}{2013}{{Cohn and Umans}}{{}}}
\bibcite{de2000multilinear}{{10}{2000}{{De~Lathauwer et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{28}{section.8}}
\bibcite{de2003nondeterministic}{{11}{2003}{{De~Wolf}}{{}}}
\bibcite{fan2019online}{{12}{2019}{{Fan and Udell}}{{}}}
\bibcite{ganti2017learning}{{13}{2017}{{Ganti et~al.}}{{}}}
\bibcite{ganti2015matrix}{{14}{2015}{{Ganti et~al.}}{{}}}
\bibcite{ghadermarzy2018learning}{{15}{2018}{{Ghadermarzy et~al.}}{{}}}
\bibcite{ghadermarzy2019near}{{16}{2019}{{Ghadermarzy et~al.}}{{}}}
\bibcite{globerson2007euclidean}{{17}{2007}{{Globerson et~al.}}{{}}}
\bibcite{han2020optimal}{{18}{2020}{{Han et~al.}}{{}}}
\bibcite{hastie2009elements}{{19}{2009}{{Hastie et~al.}}{{}}}
\bibcite{hillar2013most}{{20}{2013}{{Hillar and Lim}}{{}}}
\bibcite{hitchcock1927expression}{{21}{1927}{{Hitchcock}}{{}}}
\bibcite{hong2020generalized}{{22}{2020}{{Hong et~al.}}{{}}}
\bibcite{hore2016tensor}{{23}{2016}{{Hore et~al.}}{{}}}
\bibcite{jain2014provable}{{24}{2014}{{Jain and Oh}}{{}}}
\bibcite{kolda2009tensor}{{25}{2009}{{Kolda and Bader}}{{}}}
\bibcite{kosorok2007introduction}{{26}{2007}{{Kosorok}}{{}}}
\bibcite{pmlr-v119-lee20i}{{27}{2020}{{Lee and Wang}}{{}}}
\bibcite{li2009brain}{{28}{2009}{{Li et~al.}}{{}}}
\bibcite{montanari2018spectral}{{29}{2018}{{Montanari and Sun}}{{}}}
\bibcite{mu2014square}{{30}{2014}{{Mu et~al.}}{{}}}
\bibcite{pmlr-v70-ongie17a}{{31}{2017}{{Ongie et~al.}}{{}}}
\bibcite{robinson1988root}{{32}{1988}{{Robinson}}{{}}}
\bibcite{shen1994convergence}{{33}{1994}{{Shen and Wong}}{{}}}
\bibcite{wang2008probability}{{34}{2008}{{Wang et~al.}}{{}}}
\bibcite{wang2017bayesian}{{35}{2017}{{Wang et~al.}}{{}}}
\bibcite{wang2018learning}{{36}{2020}{{Wang and Li}}{{}}}
\bibcite{wang2019multiway}{{37}{2019}{{Wang and Zeng}}{{}}}
\bibcite{xu2018rates}{{38}{2018}{{Xu}}{{}}}
\bibcite{yuan2016tensor}{{39}{2016}{{Yuan and Zhang}}{{}}}
\bibcite{zhang2018tensor}{{40}{2018}{{Zhang and Xia}}{{}}}
\bibstyle{apalike}
