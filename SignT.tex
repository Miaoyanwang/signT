%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Nonparametric tensor completion}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{graphicx}
%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath,amssymb}
\usepackage{amsthm}    % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage{bm}
%\usepackage{subfig}
%\usepackage[english]{babel}
%\usepackage{algorithm}
%\usepackage{appendix}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{enumitem}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}


\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{cor}{Corollary}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}

\usepackage{dsfont}
%\usepackage{algpseudocode,algorithm}
%\algnewcommand\algorithmicinput{\textbf{Input:}}
%\algnewcommand\algorithmicoutput{\textbf{Output:}}
%\algnewcommand\INPUT{\item[\algorithmicinput]}
%\algnewcommand\OUTPUT{\item[\algorithmicoutput]}
%\DeclareMathOperator*{\minimize}{minimize}

\usepackage{xr}



\newcommand*{\KeepStyleUnderBrace}[1]{%f
  \mathop{%
    \mathchoice
    {\underbrace{\displaystyle#1}}%
    {\underbrace{\textstyle#1}}%
    {\underbrace{\scriptstyle#1}}%
    {\underbrace{\scriptscriptstyle#1}}%
  }\limits
}
\usepackage{makecell}
\input macros.tex

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\def\sign{\textup{sgn}}
\def\srank{\textup{srank}}
\def\rank{\textup{rank}}
\def\caliP{\mathscr{P}_{\textup{sgn}}}
\def\risk{\textup{Risk}}

\begin{document}

\twocolumn[
\icmltitle{Beyond the Signs: Nonparametric Tensor Completion via Sign Series}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chanwoo Lee}{wisc}
\icmlauthor{Miaoyan Wang}{wisc}
\end{icmlauthorlist}

\icmlaffiliation{wisc}{Department of Statistics, University of Wisconsin -- Madison}


\icmlcorrespondingauthor{Miaoyan Wang}{miaoyan.wang@wisc.edu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Higher-order tensors, completion, sliced inverse regression, probability estimation}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution


\begin{abstract}
We consider the problem of tensor estimation from noisy observations with missing entries. A nonparametric approach to tensor completion is developed based on a new model which we coin as ``sign representable tensors.'' The model represents real-valued signal tensors using a series of sign tensors with low sign-ranks. Unlike earlier methods, the sign series representation effectively addresses both low- and high-rank signal tensors, while encompassing many existing tensor models---including CP models, Tucker models, single index models, certain hypergraphon models---as special cases. We show that the sign tensor series are theoretically characterized, and computationally solvable, by classification tasks with carefully-specified weights. The excess risk rate, estimation error bound, and sample complexity are established. The results uncover the joint contribution of statistical bias-variance errors and discretization errors. Numerical results demonstrate the robustness of our proposal over previous tensor methods.
\end{abstract}

\section{Introduction}\label{Intro}

Advantages:
(1) exactly recovers the signal tensor under a wide range of low- and high-rank tensor models; 
(2) brings the nonparametric advantages of flexibility into tensor estimation and completion; 
(3) achieves computational efficiency by leveraging classification and divide-and-conquer algorithms.

\section{Preliminaries}
We use the shorthand $[n]$ to denote the $n$-set $\{1,\ldots,n\}$ for $n\in\mathbb{N}_{+}$. We use $\otimes$ to denote the outer product of vectors, $\vnormSize{}{\mx}$ to denote the vector $2$-norm, and $\mathbf{S}^{d-1}=\{\mx\in\mathbb{R}\colon \vnormSize{}{\mx}=1\}$ to denote the $(d-1)$-dimensional unit sphere. Let $\tY\in\mathbb{R}^{d_1\times \cdots \times d_K}$ denote an order-$K$ $(d_1,\ldots,d_k)$-dimensional tensor, and $\tY(\omega)\in\mathbb{R}$ denote the tensor entry indexed by $\omega \in[d_1]\times \cdots \times [d_K]$. The Frobenius norm of $\tY$ is defined as $\FnormSize{}{\tY}=\sqrt{\sum_{\omega}\tY^2(\omega)}$. Unlike matrices, various notions of decomposition have been developed for tensors of order $K\geq 3$. The Canonical Polyadic (CP) tensor decomposition~\cite{hitchcock1927expression} for a tensor $\Theta\in\mathbb{R}^{d_1\times \cdots \times d_K}$ is defined as
\begin{equation}\label{eq:CP}
\Theta=\sum_{r=1}^R\lambda_r \ma^{(1)}_r\otimes\cdots\otimes \ma^{(K)}_r,
\end{equation}
where $\lambda_1\geq \cdots \geq \lambda_R>0$ are called tensor singular values, and $\ma^{(k)}_r \in \mathbf{S}^{d_k-1}$ are called tensor singular vectors, for all $r\in[R]$, $k\in[K]$. The minimal $R$ for which the decomposition~\eqref{eq:CP} holds is called the tensor rank, denoted as $\rank(\Theta)$. 

We use $\sign(\cdot)\colon \mathbb{R}\to\{-1,1\}$ to denote the sign function, where $\sign(y)=1$ if $y\geq 0$ and $-1$ otherwise. We allow univariate functions, such as $\sign(\cdot)$ or general $f\colon \mathbb{R}\to\mathbb{R}$, to be applied to tensors in an element-wise manner. For a tensor $\Theta\in\mathbb{R}^{d_1\times \cdots \times d_K}$, its sign pattern $\sign(\Theta)$ is an order-$K$ $(d_1,\ldots,d_K)$-dimensional binary tensor with entries in $\{-1,1\}$. 



\section{Motivation and method overview}

Let $\tY$ be an order-$K$ $(d_1,\ldots,d_K)$-dimensional data tensor. Assume that $\tY$ is generated from the following model,
\begin{equation}\label{eq:model}
\tY=\Theta+\tE,
\end{equation}
where $\Theta\in\mathbb{R}^{d_1\times \cdots \times d_K}$ is the unknown signal tensor of interest, and $\tE$ is a noise tensor consisting of mean-zero, independent but not necessarily identically distributed entries. We allow heterogenous noise in that the marginal distribution of noise entry $\tE(\omega)$ may depend on $\omega$. This incorporates, for example, a Bernoulli tensor whose entries $\tY(\omega)$ have mean $\Theta(\omega)$ and variance $\text{var}(\tE(\omega))=\Theta(\omega)(1-\Theta(\omega))$. In general, we assume $\tY(\omega)$ take values in a bounded interval $[-L, L]$; other than that, we make no particular parametric assumptions on the distribution of $\tE(\omega)$.
%Unless otherwise specified, we assume $L=1$ throughout the paper. 

\begin{figure*}[h!]
\centerline{\includegraphics[width=1\textwidth]{image_new2.pdf}}
\caption{Illustration of our proposed method. For visualization purpose, we plot an order-2 tensor (a.k.a. matrix) in the figure; similar procedure applies to higher-order tensors. (a): input tensor $\tY_\Omega$ with noisy and incomplete entries. (b) and (c): our method uses weighted classification to estimate sign tensors $\sign(\Theta-\pi)$ for a sequence of levels $\pi\in \{-1,\ldots,-{1\over H},0,{1\over H},\ldots,1\}$. (d) output tensor $\hat \Theta$ with denoised and imputed entries. The depicted example is based on Example 5, where the true signal matrix has full rank. }\label{fig:demo}
\end{figure*}

Our observation is an incomplete data tensor from~\eqref{eq:model}, denoted $\tY_\Omega$, where $\Omega\subset[d_1]\times\cdots\times[d_K]$ is the index set of observed entries. We consider a general model on $\Omega$ that allows both uniform and non-uniform samplings. Specifically, let $\Pi=\{\pi_\omega\}$ be an arbitrarily predefined probability distribution over the full index set with $\sum_{\omega\in[d_1\times \cdots \times d_K]}\pi_\omega=1$. We assume the entries $\omega$ in $\Omega$ are i.i.d.\ draws with replacement from the full index set using distribution $\Pi$. The sampling rule will be denoted as $\omega\sim \Pi$.

Our goal is to accurately estimate $\Theta$ from the incomplete, noisy observation $\tY_\Omega$. In particular, we focus on the following two problems:
\begin{itemize}[leftmargin=*]
\item Q1 [Nonparametric tensor estimation]. How to flexibly estimate $\Theta$ under a wide range of structures, including both low-rankness and high-rankness?
\item Q2 [Tensor completion]. How many observed tensor entries do we need to consistently estimate the signal $\Theta$?
\end{itemize}


\subsection{Inadequacies of common low-rank models}\label{sec:example}
The signal plus noise model~\eqref{eq:model} is common in tensor literature. Most existing methods perform estimation based on the low-rankness of $\Theta$~\cite{anandkumar2014tensor,montanari2018spectral,kadmon2018statistical,cai2019nonconvex}. While these methods have shown great success in low-rank recovery, little is explored when the underlying signal tensor is of high rank. Here we provide two examples to illustrate the limitation of classical low-rank models.

The first example reveals the sensitivity of tensor rank to order-preserving transformations. Let $\tZ \in \mathbb{R}^{30\times 30\times 30}$ be an order-3 tensor with $\text{rank}(\tZ)=3$. Suppose a monotonic transformation $f(z)=(1+\exp(-cz))^{-1}$ is applied to $\tZ$ entrywise, and we observe data from model~\eqref{eq:model} with the signal tensor $\Theta=f(\tZ)$. Figure~\ref{fig:example}(a) plots the numerical rank of $\Theta$ versus $c$. Note that a smaller $c$ implies an approximate linear transformation $f(z)\approx -cz$, whereas a larger $c$ implies a higher nonlinearity $z\mapsto \{0,1\}$. As we see, the rank increases rapidly with $c$, rending traditional low-rank tensor methods ineffective even in the presence of mild order-preserving nonlinearities. In applications of digital processing and genomics analysis, the tensor of interest often undergoes some unknown transformation prior to measurements. The sensitivity to transformation therefore makes the low-rank model less desirable in practice. 

The second example shows that the classical low-rankness may exclude important specially-structured tensors. Here we consider the signal tensor of the form $\Theta=\log(1+\tZ)$, where $\tZ$ is an order-3 tensor with entries $\tZ(i,j,k)={1\over d}\max(i,j,k)$ for $(i,j,k)\in[d]^3$. In this case neither $\Theta$ nor $\tZ$ is low-rank; indeed, both tensors have rank lower bounded by dimension $d$ as illustrated in Figure~\ref{fig:example}(b) (proofs in Appendix). The matrix analogy of this $\Theta$ was studied in~\citet{chan2014consistent} in the context of graphon analysis. However, classical low-rank models fail to address these types of structures. 
\begin{figure}[h]
\includegraphics[width=.47\textwidth]{example_comb.pdf}
\caption{(a) Numerical rank of $\Theta=f(\tZ)$ versus $c$ in Example 1. Here, the numerical rank is computed as the minimal rank for which the relative least-squares error is below 0.1, and $\tZ$ is a rank-3 tensor with i.i.d.\ $N(0,1)$ entries in the (unnormalized) singular vectors. Reported ranks are averaged across 10 replicates of $\tZ$, with standard errors given in error bars. (b) Top $d=30$ tensor singular values in Example 2. Numerical values in both figures are obtained by running CP decomposition with random initialization.}\label{fig:example}
\end{figure}
In the above and many other examples, the signal tensors $\Theta$ exhibit high rank in spite of the special structures they admit. These structures are hardly detectable using classical low-rank models. New methods that allow flexible tensor modeling have yet to be developed. 

\subsection{Overview of our proposal}
Before describing our main results, we provide the intuition behind our method. In the earlier two examples, the high-rankness in the signal $\Theta$ makes the efficient estimation challenging. However, if we examine the sign of the $\pi$-shifted signal, $\sign(\Theta-\pi)$, for an arbitrary $\pi$ in the range of observations, then these sign tensors exhibit easily detectable low-rankness. For instance, the signal tensor in example 1 has the same sign pattern as a rank-$4$ tensor, since $\sign(\Theta-\pi)=\sign(\tZ-f^{-1}(\pi))$. The signal tensor in example 2 has the same sign pattern as a rank-2 block tensor~\cite{wang2019multiway}, since $\sign(\Theta-\pi)=\sign(\max(i,j,k)-d(1-e^{\pi}))$.

The above observation suggests a general framework to analyze both low- and high-rank signal tensors. Figure~\ref{fig:demo} illustrates the main crux of our method. We dichotomize the data tensor into a series of sign tensors, $\sign (\tY_\Omega-\pi)$, for $\pi\in \tH=\{-1,\ldots, {\scriptstyle -{1\over H}},0,{\scriptstyle {1\over H}},\ldots,1\}$, and then estimate the sign signals, $\sign(\Theta-\pi)$, by performing classification
\[
\hat \tZ_\pi=\argmin_{\text{low rank tensor $\tZ$}} L(\sign(\tZ),\ \sign (\tY_\Omega-\pi)).
\]
Here, $L(\cdot,\cdot)$ denotes a carefully-designed classification objective function which will be described in later sections. The final proposed tensor estimate is 
\[
\hat \Theta = {1\over 2H+1}\sum_{\pi \in \tH} \sign(\hat \tZ_\pi).
\]
Our approach is built on the nonparametric sign representation of signal tensors, and the estimate $\hat \Theta$ is essentially estimated from dichotomized tensor series $\{\sign(\tY_\Omega-\pi)\colon \pi \in \tH\}$. Surprisingly, we show that proper analysis based on dichotomized data not only preserves all information in the original signals, but also brings benefits of accuracy and flexibility over classical low-rank models. The method enjoys both statistical effectiveness and computational efficiency. 


\section{Sign representable tensors}\label{sec:representation}
In this section, we develop sign representable tensor models. The algebraic and statistical characterization of sign tensor series provides the accuracy guarantee for our method. 

\subsection{Sign-rank and sign tensor series}
Let $\Theta$ be a real-valued tensor, and $\sign (\Theta)$ be the corresponding sign patten. The sign pattern induces an equivalence relationship between tensors. Two tensors are called sign equivalent, denoted $\simeq$, if they share the same sign patterns. 

\begin{defn}[Sign-rank]
The sign-rank of a tensor $\Theta\in\mathbb{R}^{d_1\times \cdots \times d_L}$ is the minimal rank among all tensors that share the same sign patterns as $\Theta$; i.e.,
\[
\srank(\Theta) = \min \{\rank(\Theta')\colon  \Theta'\simeq \Theta,\ \Theta'\in\mathbb{R}^{d_1\times \cdots \times d_K}\}.
\]
\end{defn}
The sign-rank is also called \emph{support rank}~\cite{cohn2013fast}, \emph{minimal rank}~\cite{alon2016sign}, and \emph{nondeterministic rank}~\cite{de2003nondeterministic} in the filed of combinatorics and information theory. Earlier work defines sign-rank for binary tensors/matrices only; we extend the notion to real-valued tensors. Note that the sign-rank concerns only the sign pattern but discards the magnitudes information of $\Theta$. In particular, $\srank(\Theta)=\srank(\sign \Theta)$. 

Like most tensor problems~\cite{hillar2013most}, determining the sign-rank for a general tensor is NP hard~\cite{alon2016sign}. Fortunately, tensors arisen in application often possess special structures that facilitate analysis. We show that the family of low sign-rank tensors is strictly broader than usual low-rank tensors. This is because sign-rank is upper bounded by the usual rank. More generally, 
\begin{cor}[Upper bounds of sign-rank]~\label{cor:monotonic} For any monotonic function $g\colon \mathbb{R}\to \mathbb{R}$ with $g(0)=0$, 
\[
\textup{srank}(\Theta)\leq\rank(g(\Theta)).
\]
\end{cor}
Conversely, the sign-rank can be dramatically smaller than the usual rank, as we have shown in Section~\ref{sec:example}.
\begin{prop}[Broadness]\label{prop:extention}For every order $K\geq 2$ and dimension $d$, there exist order-$K$ $(d,\ldots,d)$-dimensional tensors $\Theta$ such that $\rank(\Theta)=d$ and $\srank(\Theta)=2$.
\end{prop}
We provide several additional examples in Appendix where the tensor rank grows with dimension $d$ but the sign-rank remains a constant. The results highlight the advantages of using sign-rank in the high-dimensional tensor analysis. Corollary~\ref{cor:monotonic} and Proposition~\ref{prop:extention} together demonstrate the strict broadness of low sign-rank tensor family over the usual low-rank tensor family. 

We are now ready to introduce a family of tensors, which we coin as ``sign representable tensors'', as the signal tensors in model~\eqref{eq:model}. 
Without loss of generality, assume tensor entries are bounded by $L=1$. 
\begin{defn}[Sign representable tensors] 
Fix a level $\pi\in[-1,1]$. A tensor $\Theta$ is called $(r,\pi)$-sign representable, if the tensor $(\Theta-\pi)$ has sign-rank bounded by $r$. A tensor $\Theta$ is called $r$-globally sign representable, if $\Theta$ is $(r,\pi)$-sign representable for all $\pi\in[-1,1]$. The collection $\{\sign(\Theta-\pi)\colon \pi \in[-1,1]\}$ is called the sign tensor series. 
We use $\caliP(r)=\{\Theta\colon \srank(\Theta-\pi)\leq r \text{ for all }\pi\in[-1,1]\}$ to denote the $r$-globally sign representable tensor family.
\end{defn}
We show that the $r$-globally sign representable tensor family is a general model that incorporates most existing tensor models, including low-rank tensors, single index models, GLM models, and certain hypergraphon models. 

\begin{example}[CP/Tucker low-rank models] The CP and Tucker low-rank models are the two most popular tensor models~\cite{anandkumar2014tensor,montanari2018spectral,kadmon2018statistical,cai2019nonconvex}. Let $\Theta$ be a low-rank tensor with CP rank $r$. Then $\Theta$ belongs to the $r$-sign representable family, i.e., $\Theta\in\caliP(r)$. Similarly, Tucker low-rank tensors $\Theta\in\caliP(r)$, where $r=\prod_kr_k$ with $r_k$ being the $k$-th mode Tucker rank of $\Theta$.  
\end{example} 

\begin{example}[Generalized linear models (GLMs)] Let $\tY$ be a binary tensor from a tensor logistic model~\cite{wang2018learning} with mean $\Theta=\mathbb{E}(\tY)=\text{logit}(\tZ)$, where $\tZ$ is a latent low-rank tensor. Then, $\Theta$ is a special (parametric) case of sign representable tensors. Same conclusion holds for general exponential-family tensors with (known) distribution-specific link functions~\cite{hong2020generalized}.
\end{example}

\begin{example}[Single index models (SIMs)] Single index model is a flexible semiparametric model initially proposed in economics~\cite{robinson1988root} and has recently been popular in high-dimensional statistics~\cite{balabdaoui2019least,ganti2017learning,alquier2013sparse}. We here extend the model to high-dimensional tensors $\Theta$. The SIM assumes the existence of a (unknown) monotonic function $g\colon \mathbb{R}\to \mathbb{R}$ such that $g(\Theta)$ has rank $r$. We see that $\Theta$ belongs to the sign representable family; i.e., $\Theta\in \caliP(r+1)$. 
\end{example}

\begin{example}[Tensor block models (TBMs)] Tensor block model~\cite{wang2019multiway,chi2020provable} assumes a checkerbord structure among tensor entries under marginal index permutation. The signal tensor $\Theta$ takes at most $r$ distinct values, where $r$ is the total number of multiway blocks. Our model incorporates TBM because $\Theta \in \caliP(r)$. 
\end{example}

\begin{example}[Min/Max hypergraphon] Graphon is a popular nonparametric model for networks~\cite{chan2014consistent,xu2018rates}, and we have extended the model for tensors in Section~\ref{sec:example}. Here we revisit the model for generality. Let $\Theta$ be an order-$K$ tensor generated from the hypergraphon $\Theta(i_1,\ldots,i_K)=\log(1+\max_kx^{(k)}_{i_k})$, where $x^{(k)}_{i_k}\sim \text{Unif}[0,1]$ i.i.d.\ for all $i_k\in[d_k]$ and $k\in[K]$. Every sign tensor $\sign(\Theta-\pi)$ in the series of $\pi\in[0,\ \log 2]$ is a block tensor with at most two blocks, so $\Theta \in \caliP(2)$. 

More generally, let $g(\cdot)$ be a continuous univariate function with at most $r$ real-valued roots in the equation $g(z)=\pi$; this property holds, e.g., when $g(z)$ is a polynomial of degree $r$. Then, the tensor $\Theta$ generated from $\Theta(i_1,\ldots,i_K)=g(\max_kx^{(k)}_{i_k})$ belongs to $\caliP(r+1)$. Same conclusion applies if the maximum is replaced by minimum.
\end{example}

\subsection{Statistical characterization of sign tensors via weighted classification}

Accurate estimation of a sign representable tensor crucially depends on the behavior of sign tensor series $\sign(\Theta-\pi)$. In this section, we show that weighted classification completely characterizes the sign tensors. The results bridge the algebraic and statistical properties of sign representable tensors, thereby providing the theoretical guarantee for our nonparametric algorithm (Figure~\ref{fig:demo}).
 
 For a given $\pi \in [-1,1]$, define a $\pi$-shifted data tensor $\bar \tY = (\tY-\pi)$. We introduce a weighted classification objective function
\begin{equation}
L(\tZ, \bar \tY)= {1\over |\Omega|}\sum_{\omega \in \Omega}\KeepStyleUnderBrace{|\bar \tY(\omega)|}_{\text{entry-specific weight}} \times \KeepStyleUnderBrace{| \sign \tZ(\omega)-\sign \bar \tY(\omega)|}_{\text{classification loss}},
\end{equation}
where $\tZ\in\mathbb{R}^{d_1\times \cdots \times d_K}$ is the decision variable to be optimized, $|\bar \tY(\omega)|$ is the entry-specific weight equal to the distance from the tensor entry to the target level $\pi$. In the special case when the data tensor entries are binary in $\{-1,1\}$ and the target level $\pi=0$, the objective reduces to the usual unweighted classification loss.

The specification of entry-specific weight is important in characterizing $\sign(\Theta-\pi)$, as we show now. Define the weighted classification risk 
\begin{equation}\label{eq:population}
\textup{Risk}(\tZ)=\mathbb{E}_{\tY,\Omega}L(\tZ,\bar\tY),
\end{equation}
where the expectation is taken with respect to both $\tY$ under model~\eqref{eq:model} and $\Omega$ under the sampling distribution $\omega\sim\Pi$. Note that the form of $\textup{Risk}(\cdot)$ implicitly depends on $\pi$; we suppress $\pi$ when no confusion arises. 
\begin{prop}[Global optimum of weighted risk]\label{prop:global}
Suppose the data $\tY_\Omega$ is generated from model~\eqref{eq:model} with $\Theta \in \caliP(r)$. Then, for all $\bar \Theta$ that are sign equivalent to $\sign(\Theta-\pi)$, 
\begin{align}
\textup{Risk}(\bar \Theta )&=\inf\{\textup{Risk}(\tZ)\colon \tZ\in\mathbb{R}^{d_1\times \cdots \times d_K}\},\\
&=\inf\{\textup{Risk}(\tZ)\colon \textup{rank} \tZ\leq r\}.
\end{align}
\end{prop}
The results show that the sign tensor $\sign(\Theta-\pi)$ is (one of) the optimizer for the weighted classification risk. The weight allows .... 


This fact suggests $\sign(\Theta-\pi)$ by optimizing $L(\tZ,\bar \tY)$. 

%The sign tensor $\sign(\Theta-\pi)$ serves the bridge between the target parameter $\Theta$ and statistical properties of $\textup{Risk}(\cdot)$. 
%The above property shows that the $\sign(\Theta-\pi)$ is one of the optimum of $\risk(\cdot)$. 
In general, the converse may not true. To establish the accuracy, we also need to establish the uniqueness; that is, is the tensor $\sign(\Theta-\pi)$ the unique (up to sign equivalence) optimizer of $\risk(\cdot)$? The following assumption quantifies the identifiability of $\bar \Theta$ from $\risk(\cdot)$. 


%Suppose the entries in the index set $\Omega$ are i.i.d.\ sampled with replacement from the entire index set using a probability $\Pi$ over $[d_1]\times\cdots\times[d_K]$. We use $\omega\sim \Pi$ for $\omega\in[d_1]\times\cdots\times[d_K]$ to denote this observation model. 
%The underlying tensor $\Theta$ and the sampling distribution $\omega\sim \Pi$ induces a [-1,1]-valued random variable $\Theta(\omega)$. We use $\mathbb{P}_{\omega\sim \Pi}$ to denote the distribution law over for $\Theta(\omega)$. 
%\begin{enumerate}
%\item $(\omega)\sim\Pi$
%\item Marginal distribution $\Theta(\omega) & \sim \tF, \ \text{for each }\omega\in[d_1]\times\cdots\times[d_K]$,\\
%\item Jointly $\srank(\Theta-\pi)& \leq r, \ \text{for }\pi\in[0,1]$
%\end{enumerate}

We introduce some additional notation. Recall that $\omega\in \Pi$ denotes the sampling distribution over index set. Our theory concerns the high-dimensional ... Both $\Pi$ and $\Theta$ implicitly depend on the tensor dimension. We are interested in the high dimensional region $d:=\min_kd_k\to\infty$ for the sequence $\Pi=\Pi_d$ and $\Theta=\Theta_d$. Unless otherwise stated, all relevant assumptions should be interpreted as uniform conditions that hold for all $d$. 
%Let $\hat F(t)={1\over d^K}\sum_{\omega\in[d]^K}\mathds{1}(\Theta_d(\omega)\leq t)$ be the marginal empirical cumulative distribution function (CDF) of signal entries. Assume $\hat F(t)\to F(t)$ for every value of $t$ as $d\to\infty$. 
Let $\tN=\{\pi\colon \mathbb{P}_{\omega\sim \Pi}(\Theta(\omega)=\pi)\neq 0\}$ denote the set of mass points of $\Theta(\omega)$. Assume there exists a constant $c_1>0$, independent of tensor dimension, such that $|\tN|\leq c_1$. Furthermore, 

\begin{assumption}[$\alpha$-smoothness]\label{ass:margin} 
Fix $\pi\notin \tN$. Assume there exist constants $\alpha=\alpha(\pi), c=c(\pi) >0$, independent of tensor dimension, such that, 
\begin{equation}\label{eq:smooth}
\sup_{0\leq t<\rho(\pi, \tN)}{\mathbb{P}_{\omega \sim \Pi}(\omega \colon |\Theta (\omega)-\pi|\leq t )\over t^\alpha} \leq c,
\end{equation}
where $\rho(\pi,\tN):=\min_{\pi'\in \tN}|\pi-\pi'|$ denotes the distance from $\pi$ to the nearest point in $\tN$. The largest possible $\alpha=\alpha(\pi)$ is called the smoothness index at level $\pi$. We make the convention that $\alpha= \infty$ if the set $\{\omega\colon |\Theta(\omega)-\pi|\leq t\}$ is measure-zero set, implying the density of $\Theta(\omega)$ has a jump around the level $\pi$.
\end{assumption}

If $\Theta$ is $\alpha$-smooth for all $\pi\in[-1,1]$ except for a finite number of points, then we call $\Theta$ is $\alpha$-globally smooth. 
The smoothness index is determined by the marginal density of $\Theta(\omega)$, or equivalently, by the underlying $\Theta$ and the sampling distribution $\omega\sim \Pi$. 

To gain some intuition about~\eqref{eq:smooth}, we consider the case of uniform sampling model where each tensor entry has equal probability to be observed. Larger value of $\alpha$ corresponds to an easier problem (faster statistical rate). The case $\alpha<1$ may indicate a local extremum or inflection point of the density of $\Theta(\omega)$ at the level $\pi$. Typical cases are $\alpha=1$, for example, when the density of $\Theta(\omega)$ is locally bilipschitz. 

For tensor block models, $|\tN|$ equals the number of blocks with distinct means which is finite. Furthermore, we can take $\alpha= \infty$ for all $\pi \notin \tN$ (because the numerator in~\eqref{eq:smooth} is zero). For max/min hypergraphon model with $r$-th polynomial function, we have $\alpha=1$ for all $\pi$ except at most $r$ many points (local extremums). For single index model $f(\Theta)$....

We consider the mean absolute error (MAE)
\[
\text{MAE}(\Theta_1, \Theta_2)\stackrel{\text{def}}{=}\mathbb{E}_{\omega\sim \Pi}\onenormSize{}{\Theta_1-\Theta_2}.
\]

We now reach the main theorem in this section. 
\begin{thm}[Identifiability] Under Assumption~\ref{ass:margin}, for tensors $\bar \Theta \simeq \sign(\Theta-\pi)$ and all tensors $\tZ\in\mathbb{R}^{d_1\times \cdots \times d_K}$,
\[
\textup{MAE}(\sign \tZ, \sign \bar \Theta) \leq C(\pi)\left[\textup{Risk}(\tZ)-\textup{Risk}( \bar \Theta)\right]^{\alpha/(1+\alpha)},
\]
where $C(\pi)>0$ is a constant. 
\end{thm}
The result immediately suggests uniqueness of the optimizer of $\text{Risk}(\cdot)$ up to a zero-measure set under $\Pi$. Furthermore, it establish the stability of recovering $\sign \Theta$ from the optimization~\eqref{eq:population}.  
%Second, the empirical optimizers is still non-unique, because any tensor $\hat \tZ'$ that share the same sign pattern as $\hat \tZ$ are the solution. To make the optimization well posed, we propose the following minimal-norm optimization



\section{Nonparametric tensor completion via sign series}

Now, we summarize the completion algorithm and accuracy guarantees. We use the sample formulation ... , and propose the estimate
\begin{equation}\label{eq:estimate}
\hat \tZ_\pi = \argmin_{\tZ\colon \text{rank}(\tZ)\leq r, \ \FnormSize{}{\tZ}\leq 1} L(\tZ,\bar \tY_\pi),
\end{equation}
for a series of levels $\pi \in \tH=\{-1,\ldots,-{1\over H}, 0, {1\over H},\ldots,1\}$, and the aggregated tensor estimate
\[
\hat \Theta = {1\over 2H+1}\sum_{\pi \in \tH}\sign{\hat \tZ_\pi}.
\]
%In order to establish the accuracy of $\hat Z$ to $\bar \Theta$. 
%\begin{equation}\label{eq:estimate}
%\hat \tZ_\lambda = \argmin_{\tZ\in \caliP(r)} L(\tZ, \tY).
%\end{equation}
%Here $\lambda$ is a tuning parameter; when $\lambda \to 0$, the solution converges to the minimal-norm optimizer. %Let $\Pi=\{\pi_\omega\}$ denote the sampling distribution over the index set $d_1\times \cdots \times d_K$ such that $\sum_{\omega}\pi_\omega=1$. 
%We define the weighted classification error (CErr) between two binary tensors $\tA, \tB \in \{-1,1\}^{d_1\times \cdots \times d_K}$.
%\[
%\text{CErr}(\tA,\tB)=\mathbb{E}_{\Omega \sim \Pi}\onenormSize{}{\tA-\tB}=\sum_{\omega}\pi_\omega|\tA(\omega)-\tB(\omega)|.
%\]

%\begin{assumption}\label{ass} Fix a $\pi\in[-L,L]$. Assume there exist constants $\delta=\delta(\pi)>0$ and $C=C(\pi)>0$, independent of tensor dimension, such that
%\[
%\mathbb{P}_{\Omega\sim \Pi}(\omega \colon |\bar \Theta (\omega)|\leq t ) \leq Ct^\alpha, \ \text{for all }t\in(0,\delta).
%\]
%\end{assumption}
\begin{thm}[Estimation of sign series]\label{thm:classification} Suppose $\Theta\in\caliP(r)$ and Assumption~\ref{ass:margin} holds at level $\pi\notin \tN$ with smoothness index $\alpha\in[0,1]$. Then, for every such $\pi$, with very high probability over $\tY_\Omega$, 
\[
\textup{MAE}(\sign \hat \tZ_\pi, \sign \bar \Theta_\pi) \lesssim \left({d r \over |\Omega|}\right)^{\alpha/(\alpha+2)}+{1\over \rho^2(\pi, \tN)} {dr \over |\Omega|}.
\]
\end{thm}
Theorem~\ref{thm:classification} shows that the estimate from~\eqref{eq:estimate} recovers the desired sign tensor $\sign \bar \Theta$. 
%\begin{cor}[Excess risk and generalization error]
%\[
%\risk(\hat \tZ_\pi)-\risk(\bar \Theta) \leq C(\pi)\left({d r \over |\Omega|}\right)^{(\alpha+1)/(\alpha+2)}.
%\]
%\end{cor}
%Compared to generalization risk, with probability $1-\delta$
%\[
%|\risk(\hat \tZ)-L(\hat \tZ, \tY)| \leq \sqrt{dr\over |\Omega|}+{\delta \over |\Omega|}.
%\]
The result suggests the sample complexity for sign tensor recovery is of the order $dr$. We will show that later, this complexity remains for recovering the entire tensor. 
%we consider the mean absolute error (MAE)
%\[
%\text{MAE}(\Theta_1, \Theta_2)\stackrel{\text{def}}{=}\mathbb{E}_{\omega\sim \Pi}\onenormSize{}{\Theta_1-\Theta_2}.
%\]
\begin{thm}[Tensor estimation error] With very high probability over $\tY_\Omega$,
\[
\textup{MAE}(\hat \Theta, \Theta)\leq \left({d r \over |\Omega|}\right)^{\alpha/(\alpha+2)}+{1\over H}+H{d r \over |\Omega|}.
\]
Setting $H\asymp \left( |\Omega|\over rd\right)^{1/2}$ gives
\[
\textup{MAE}(\hat \Theta, \Theta)\leq \left(dr \over|\Omega|\right)^{{\alpha \over \alpha+2} \vee {1\over 2}}.
\]
\end{thm}

\begin{cor}[Sample complexity for completion] With very high probability 
\[
\textup{MAE}(\hat \Theta, \Theta)\to 0, \quad \text{as}\quad {|\Omega|\over d_{\max}}\to \infty.
\]
\end{cor}

Completion is consistent provided that $|\Omega|/dr\to\infty$. Furthermore, in the full observation case, $|\Omega|=\prod_k d_k$, then the 
\[
\textup{MAE}(\hat \Theta, \Theta)\leq \left(r\over d^{-(K-1)}\right)^{{\alpha \over \alpha+2} \vee {1\over 2}}.
\]
%For tensor block model $\alpha=\infty$, we have ${1\over 2}$. For single index model ${1\over 3}$. 

\begin{example}[Tensor block model]
Consider tensor block model. $\alpha=\infty$. Earlier results show the estimation error
\[
\textup{MAE}(\hat \Theta, \Theta)\leq \sqrt{r\log d \over |\Omega|}.
\]
this is the best rate. reached the minimax rate??
\end{example}
\begin{example}[Single index tensor model]
$\tO(1/3)$???
\end{example}
%\begin{thm}[Minimax rate] Because our function class incorporates the tensor block model, the following results immediately follows from the minimax rate for tensor block model
%\[
%\inf_{\hat \Theta}\sup_{\Theta \in \caliP(r)} {1\over n^2}\mathbb{E}\FnormSize{}{\hat \Theta - \Theta} \geq {rd\over |\Omega|},
%\]
%when $\alpha=1$. 
%\end{thm}

%\section{Algorithm}
The optimization~\eqref{eq:estimate} is a non-convex problem because of the nonconvexity in $L$ and the rank constraints. We present an efficient ADMM algorithm with a good empirical performance. 

\begin{align}
\min_{\tZ}L(\tZ)&:={1\over |\Omega|}\sum_{\omega \in \Omega}|\bar y_\omega| (1-z_\omega\bar y_\omega)_{+}\\
\text{subject to }&\rank(\tZ)\leq r \ \text{and }\FnormSize{}{\tZ}\leq 1.
\end{align}

\begin{align}
L(\tZ,\tW,\Lambda,\rho,\lambda)&={1\over |\Omega|}\sum_{\omega \in \Omega}|\bar y_\omega| (1-z_\omega\bar y_\omega)_{+}\\
&+\lambda\FnormSize{}{\tZ}^2+
\rho \FnormSize{}{\tZ-\tW}^2 + \langle \tZ-\tW,\Lambda \rangle
\end{align}
Two changes: 1. make convex relaxation; 2. alternatively... Gradually increasing $\rho$ and $\lambda$.


\clearpage
\begingroup
\let\clearpage\relax 
\onecolumn 
%\begin{prop}
%Let $\Theta=\sum_{r=1}^d\me_r^{\otimes 3}$ be the identity tensor, where $\me_r=(0,\ldots,0,1,0,\ldots,0)^T$ is the $r$-th canonical basis in $\mathbb{R}^d$. Then $\rank(\Theta)=d$ and $\srank(\Theta)=2$. 
%\end{prop}
\begin{proof}[Proof of Proposition~\ref{prop:extention}]
We define $\Theta=\sum_{r=1}^d \me^{\otimes 2}_r\otimes \mathbf{1}^{\otimes (K-2)}_d$, where $\me_r=(0,\ldots,0,1,0,\ldots,0)^T$ is the $r$-th canonical basis in $\mathbb{R}^d$, and $\mathbf{1}_d\in\mathbb{R}^d$ is a vector with all entries 1. Based on the definition of $\Theta$, we have
\[
\rank(\Theta)=\rank(\mI),\quad \srank(\Theta)=\srank(\mI),
\]
where $\mI\in\mathbb{R}^{d\times d}$ is the identity matrix. Therefore, it suffices to show that $\srank(\mI)=3$. We now construct a rank-2 matrix $\mA$ such that $\sign(\mA-1/2)=\sign(\mI)$. Define
\[
\mA=\begin{bmatrix}
1 & -{1\over 2}\times 1\\
2^{-1}& -{1\over 2}\times 4^{-1}\\
\vdots&\vdots\\
2^{-d+1}& -{1\over 2}\times 4^{-d+1}
\end{bmatrix}
\begin{bmatrix}
1&2&\cdots&2^{d-1}\\
1&4&\cdots& 4^{d-1}
\end{bmatrix}.
\]
It is easy to verify that $\mA(i,j)={1\over 2}$ if $i=j$, and $\mA(i,j)<{1\over 2}$ otherwise. Therefore, $\sign(\mA-1/2)=\mI$. 
\end{proof}
\begin{proof}[Proof of Proposition~\ref{prop:global}]
Based on the definition, the function $\risk(\cdot)$ relies only on the sign pattern of the tensor. Therefore, without loss of generality, we assume both tensors $\bar \Theta, \tZ \in\{-1,1\}^{d_1\times \cdots \times d_K}$ are binary tensors. 
We evaluate the excess risk 
\begin{equation}\label{eq:risk}
\risk(\tZ)- \risk(\bar \Theta) = \mathbb{E}_{\omega\sim \Pi}\KeepStyleUnderBrace{\mathbb{E}_{\tY(\omega)}\left\{|\tY(\omega)-\pi|\left[\left|\tZ(\omega)-\sign(\bar \tY(\omega)) \right|-\left|\bar\Theta(\omega)-\sign(\bar \tY(\omega))\right|\right]\right\}}_{=:I(\omega)}.
\end{equation}
Denote $y=\tY(\omega)$, $z=\tZ(\omega)$, $\bar \theta=\bar\Theta(\omega)$, and $\theta=\Theta(\omega)$. It follows from the expression of $I(\omega)$ that
\begin{align}\label{eq:I}
I(\omega)&= \mathbb{E}_{y}\left[ (y-\pi)(\bar \theta-z)\mathds{1}(y\geq \pi)+(\pi-y)(z-\bar \theta)\mathds{1}(y< \pi)\right]\\
&= \mathbb{E}_{y}\left[(\bar \theta-z) (y-\pi)\right]\\
&=  \left[\sign(\theta-\pi)-z\right]\left(\theta-\pi\right)\\
&= |\sign(\theta-\pi)-z||\theta-\pi|\geq 0
\end{align}
where the third line uses the fact $\mathbb{E}y=\theta$ and $\bar \theta=\sign(\theta-\pi)$, and the last line uses the assumption $z \in\{-1,1\}$. In particular, the equality is attained when $z=\sign(\theta-\pi)$ or $\theta=\pi$. Combining~\eqref{eq:I} with~\eqref{eq:risk}, we conclude 
\[
\risk(\tZ)- \risk(\bar \Theta) = \mathbb{E}_{\omega\sim \Pi} |\sign(\Theta(\omega)-\pi)-\tZ(\omega)||\Theta(\omega)-\pi|\geq 0,
\]
for all $\tZ\in\{-1,1\}^{d_1\times \cdots \times d_K}$. Therefore,
\[
\risk(\bar \Theta)=\min\{\risk(\tZ)\colon \tZ\in \mathbb{R}^{d_1\times \cdots \times d_K}\} \leq \min\{\risk(\tZ)\colon \rank(\tZ)\leq r\}.
\]
Because $\srank(\bar \Theta)\leq r$ by assumption, the last inequality becomes equality. The proof is complete. 
\end{proof}


\begin{proof}
We verify two conditions. 
\begin{enumerate}
\item Approximation error. For $\tZ$ with $\text{rank}(\tZ)\leq r$, we have $\textup{Risk}(\tZ)-\textup{Risk}(\bar \Theta)=0$ for all $d$.  
\item Variance-to-mean relationship
\[
\text{Var}_{\tY, \Omega}[L(\tZ,\bar \tY_\pi)-L(\bar \Theta, \tY_\pi)]\leq [\textup{Risk}(\tZ)-\textup{Risk}(\bar \Theta)]^{\alpha/(1+\alpha)}+{1\over \rho(\pi, \tN)}[\textup{Risk}(\tZ)-\textup{Risk}(\bar \Theta)].
\]
Apply Lemma~\ref{lem:tensor} to the above condition, we obtain
\[
\textup{Risk}(\tZ)-\textup{Risk}(\bar \Theta)\leq t_n^{(\alpha+1)/(\alpha+2)}+{1\over \rho(\pi, \tN)}t_n, \quad \text{where }t_n={Krd\over n}.
\]
\end{enumerate}
\end{proof}

\begin{lem}\label{lem:tensor}

Because the classification rate is scale-free; $\text{Risk}(\tZ)=\text{Risk}(c\tZ)$ for every $c>0$. Therefore, without loss of generality, we solve the estimate subject to $\FnormSize{}{\tZ}\leq 1$,
\[
\hat \tZ=\argmin_{\tZ\colon \textup{rank}(\tZ)\leq r, \FnormSize{}{\tZ}\leq 1}L(\tZ,\bar \tY_\pi).
\]
Write $|\Omega|=n$. We have
\[
\mathbb{P}[\textup{Risk}(\hat \tZ)-\textup{Risk}(\bar \Theta)\geq t_n]\leq {7\over 2}\exp(-Cnt_n).
\]
The rate of convergence $t_n>0$ is determined by the solution to the following inequality,
\[
{1\over t_n}\int^{\sqrt{t_n^\alpha+\rho^{-1}t_n}}_{t_n} \sqrt{\tH_{[\ ]}(\varepsilon,\ \tF,\ \vnormSize{}{\cdot}) }d\varepsilon \leq n^{1/2}, 
\]
where $\tF=\{\tZ\colon \textup{rank}(\tZ)\leq r, \ \FnormSize{}{\tZ}^2\leq 1\}$ and $\rho=\rho(\pi, \tN)$. By Lemma~\ref{lem:bracketing}, we obtain 
\[
t_n\asymp \left({Kdr\over n}\right)^{(\alpha+1)/(\alpha+2)} +{1\over \rho^2(\pi, \tN)} {Kdr\over n}.
\] 
Finally, we obtain
\[
\mathbb{P}[\textup{Risk}(\hat \tZ)-\textup{Risk}(\bar \Theta)\geq t_n]\leq {7\over 2}\exp(-Cd^{\alpha+1\over \alpha+2}n^{1\over \alpha+2}) \leq {7\over 2}\exp(-C\sqrt{d}),
\]
where $C=C(k,r)>0$ is a constant independent of $d$ and $n$.  
\end{lem}

\begin{lem}[Bracketing number for bounded low rank tensor]\label{lem:bracketing}
\[
\sqrt{\mathbb{E}_{\omega\sim \Pi}|\tZ_1(\omega)-\tZ_2(\omega)|^2} \leq \mnormSize{}{\tZ_1-\tZ_2} \leq \FnormSize{}{\tZ_1-\tZ_2}.
\]
Therefore
\[
\tH_{[\ ]}(2\varepsilon, \tF, \vnormSize{}{\cdot})\leq \tH(\varepsilon, \tF, \FnormSize{}{\cdot}) \leq C{(1+Kdr)\log {d\over \varepsilon}},
\]
where the covering number for low rank tensor is based on~\citet{mu2014square,9119759}.
\end{lem}
\endgroup


\bibliographystyle{chicago}

\bibliography{tensor_wang}

\appendix

\end{document}
