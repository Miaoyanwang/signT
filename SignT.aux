\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hitchcock1927expression}
\newlabel{Intro}{{1}{1}{}{section.1}{}}
\newlabel{eq:CP}{{1}{1}{}{equation.2.1}{}}
\MT@newlabel{eq:CP}
\newlabel{eq:model}{{2}{1}{}{equation.3.2}{}}
\citation{anandkumar2014tensor,montanari2018spectral,kadmon2018statistical,cai2019nonconvex}
\citation{chan2014consistent}
\newlabel{fig:demo}{{1}{2}{Illustration of our proposed method. For visualization purpose, we plot an order-2 tensor (a.k.a. matrix) in the figure; similar procedure applies to higher-order tensors. (a): input tensor $\tY _\Omega $ with noisy and incomplete entries. (b) and (c): our method uses weighted classification to estimate sign tensors $\sign (\Theta -\pi )$ for a sequence of levels $\pi \in \{-1,\ldots ,-{1\over H},0,{1\over H},\ldots ,1\}$. (d) output tensor $\hat \Theta $ with denoised and imputed entries. The depicted example is based on Example 5, where the true signal matrix has full rank}{figure.1}{}}
\MT@newlabel{eq:model}
\newlabel{sec:example}{{3.1}{2}{}{subsection.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{fig:example}{{2}{2}{(a) Numerical rank of $\Theta =f(\tZ )$ versus $c$ in Example 1. Here, the numerical rank is computed as the minimal rank for which the relative least-squares error is below 0.1, and $\tZ $ is a rank-3 tensor with i.i.d.\ $N(0,1)$ entries in the (unnormalized) singular vectors. Reported ranks are averaged across 10 replicates of $\tZ $, with standard errors given in error bars. (b) Top $d=30$ tensor singular values in Example 2. Numerical values in both figures are obtained by running CP decomposition with random initialization}{figure.2}{}}
\citation{cohn2013fast}
\citation{alon2016sign}
\citation{de2003nondeterministic}
\citation{hillar2013most}
\citation{alon2016sign}
\citation{anandkumar2014tensor,montanari2018spectral,kadmon2018statistical,cai2019nonconvex}
\newlabel{sec:representation}{{4}{3}{}{section.4}{}}
\newlabel{sec:sign-rank}{{4.1}{3}{}{subsection.4.1}{}}
\newlabel{cor:monotonic}{{1}{3}{Upper bounds of sign-rank}{cor.1}{}}
\newlabel{prop:extention}{{1}{3}{Broadness}{prop.1}{}}
\MT@newlabel{eq:model}
\citation{wang2018learning}
\citation{hong2020generalized}
\citation{robinson1988root}
\citation{balabdaoui2019least,ganti2017learning,alquier2013sparse}
\citation{wang2019multiway,chi2020provable}
\citation{chan2014consistent,xu2018rates}
\newlabel{eq:sample}{{3}{4}{}{equation.4.3}{}}
\newlabel{eq:population}{{4}{4}{}{equation.4.4}{}}
\MT@newlabel{eq:model}
\newlabel{prop:global}{{2}{4}{Global optimum of weighted risk}{prop.2}{}}
\MT@newlabel{eq:model}
\newlabel{eq:optimal}{{2}{4}{Global optimum of weighted risk}{prop.2}{}}
\newlabel{ass:margin}{{1}{5}{$\alpha $-smoothness}{assumption.1}{}}
\newlabel{eq:smooth}{{5}{5}{$\alpha $-smoothness}{equation.4.5}{}}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:population}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:smooth}
\newlabel{sec:estimation}{{5}{5}{}{section.5}{}}
\newlabel{eq:est}{{6}{5}{}{equation.5.6}{}}
\newlabel{eq:estimate}{{7}{5}{}{equation.5.7}{}}
\MT@newlabel{eq:sample}
\MT@newlabel{eq:estimate}
\newlabel{thm:classification}{{5.1}{5}{Estimation of sign series}{thm.5.1}{}}
\citation{ganti2015matrix}
\citation{bartlett2006convexity}
\newlabel{thm:estimation}{{5.2}{6}{Tensor estimation error}{thm.5.2}{}}
\MT@newlabel{eq:est}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:CP}
\newlabel{fig:compare1}{{4}{7}{Estimation performance under full completion}{figure.4}{}}
\newlabel{fig:compare2}{{5}{8}{Completion performance with observation}{figure.5}{}}
\newlabel{eq:risk}{{8}{9}{}{equation.6.8}{}}
\newlabel{eq:I}{{9}{9}{}{equation.6.9}{}}
\MT@newlabel{eq:I}
\MT@newlabel{eq:risk}
\citation{mu2014square,9119759}
\bibstyle{chicago}
\bibdata{tensor_wang}
\bibcite{alon2016sign}{{1}{2016}{{Alon et~al.}}{{Alon, Moran, and Yehudayoff}}}
\bibcite{alquier2013sparse}{{2}{2013}{{Alquier and Biau}}{{Alquier and Biau}}}
\bibcite{anandkumar2014tensor}{{3}{2014}{{Anandkumar et~al.}}{{Anandkumar, Ge, Hsu, Kakade, and Telgarsky}}}
\bibcite{balabdaoui2019least}{{4}{2019}{{Balabdaoui et~al.}}{{Balabdaoui, Durot, Jankowski, et~al.}}}
\bibcite{bartlett2006convexity}{{5}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{cai2019nonconvex}{{6}{2019}{{Cai et~al.}}{{Cai, Li, Poor, and Chen}}}
\bibcite{chan2014consistent}{{7}{2014}{{Chan and Airoldi}}{{Chan and Airoldi}}}
\bibcite{chi2020provable}{{8}{2020}{{Chi et~al.}}{{Chi, Gaines, Sun, Zhou, and Yang}}}
\bibcite{cohn2013fast}{{9}{2013}{{Cohn and Umans}}{{Cohn and Umans}}}
\bibcite{de2003nondeterministic}{{10}{2003}{{De~Wolf}}{{De~Wolf}}}
\bibcite{ganti2017learning}{{11}{2017}{{Ganti et~al.}}{{Ganti, Rao, Balzano, Willett, and Nowak}}}
\newlabel{lem:tensor}{{1}{10}{}{lem.1}{}}
\newlabel{lem:bracketing}{{2}{10}{Bracketing number for bounded low rank tensor}{lem.2}{}}
\bibcite{ganti2015matrix}{{12}{2015}{{Ganti et~al.}}{{Ganti, Balzano, and Willett}}}
\bibcite{hillar2013most}{{13}{2013}{{Hillar and Lim}}{{Hillar and Lim}}}
\bibcite{hitchcock1927expression}{{14}{1927}{{Hitchcock}}{{Hitchcock}}}
\bibcite{hong2020generalized}{{15}{2020}{{Hong et~al.}}{{Hong, Kolda, and Duersch}}}
\bibcite{9119759}{{16}{2020}{{{Ibrahim} et~al.}}{{{Ibrahim}, {Fu}, and {Li}}}}
\bibcite{kadmon2018statistical}{{17}{2018}{{Kadmon and Ganguli}}{{Kadmon and Ganguli}}}
\bibcite{montanari2018spectral}{{18}{2018}{{Montanari and Sun}}{{Montanari and Sun}}}
\bibcite{mu2014square}{{19}{2014}{{Mu et~al.}}{{Mu, Huang, Wright, and Goldfarb}}}
\bibcite{robinson1988root}{{20}{1988}{{Robinson}}{{Robinson}}}
\bibcite{wang2018learning}{{21}{2020}{{Wang and Li}}{{Wang and Li}}}
\bibcite{wang2019multiway}{{22}{2019}{{Wang and Zeng}}{{Wang and Zeng}}}
\bibcite{xu2018rates}{{23}{2018}{{Xu}}{{Xu}}}
