\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hitchcock1927expression}
\newlabel{Intro}{{1}{1}{}{section.1}{}}
\newlabel{eq:CP}{{1}{1}{}{equation.2.1}{}}
\MT@newlabel{eq:CP}
\newlabel{eq:model}{{2}{1}{}{equation.3.2}{}}
\MT@newlabel{eq:model}
\citation{anandkumar2014tensor,montanari2018spectral,kadmon2018statistical,cai2019nonconvex}
\citation{chan2014consistent}
\newlabel{fig:demo}{{1}{2}{Illustration of our proposed method. For visualization purpose, we plot an order-2 tensor (a.k.a. matrix) in the figure; similar procedure applies to higher-order tensors. (a): input tensor $\tY _\Omega $ with noisy and incomplete entries. (b) and (c): the algorithm uses weighted classification to estimate sign tensors $\sign (\Theta -\pi )$ for a sequence of levels $\pi \in \{-1,\ldots ,-{1\over H},0,{1\over H},\ldots ,1\}$. (d) output tensor $\hat \Theta $ with denoised and imputed entries. The depicted example is based on Example 5, where the true signal matrix has full rank}{figure.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:model}
\newlabel{fig:example}{{2}{2}{(a) Numerical rank of $\Theta =f(\tZ )$ versus $c$ in Example 1. Here $\tZ $ is generated using a rank-3 tensor whose (unnormalized) eigenvectors consist of i.i.d.\ entries from $N(0,1)$. The numerical rank is computed as the minimal $R$ such that $\min _{\rank (\tA )\leq R}{ \FnormSize {}{\Theta -\tA }\over \FnormSize {}{\Theta }} \leq 0.1$. (b). Top $d=30$ numerical singular values of tensors $\Theta $ and $\tZ $ in Example 2. The reported values in both figures are obtained by running CP decomposition ten times using random initializations}{figure.2}{}}
\citation{cohn2013fast}
\citation{de2003nondeterministic}
\newlabel{cor:monotonic}{{1}{3}{Upper bounds of sign rank}{cor.1}{}}
\newlabel{prop:extention}{{1}{3}{Strictness}{prop.1}{}}
\MT@newlabel{eq:model}
\newlabel{eq:population}{{3}{4}{}{equation.4.3}{}}
\newlabel{ass:margin}{{1}{4}{$\alpha $-smoothness}{assumption.1}{}}
\newlabel{eq:smooth}{{4}{4}{$\alpha $-smoothness}{equation.4.4}{}}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:smooth}
\MT@newlabel{eq:population}
\newlabel{eq:estimate}{{5}{5}{}{equation.5.5}{}}
\newlabel{thm:classification}{{5.1}{5}{Estimation of sign series}{thm.5.1}{}}
\MT@newlabel{eq:estimate}
\MT@newlabel{eq:estimate}
\citation{mu2014square,9119759}
\bibstyle{chicago}
\bibdata{tensor_wang}
\bibcite{anandkumar2014tensor}{{1}{2014}{{Anandkumar et~al.}}{{Anandkumar, Ge, Hsu, Kakade, and Telgarsky}}}
\bibcite{cai2019nonconvex}{{2}{2019}{{Cai et~al.}}{{Cai, Li, Poor, and Chen}}}
\bibcite{chan2014consistent}{{3}{2014}{{Chan and Airoldi}}{{Chan and Airoldi}}}
\bibcite{cohn2013fast}{{4}{2013}{{Cohn and Umans}}{{Cohn and Umans}}}
\bibcite{de2003nondeterministic}{{5}{2003}{{De~Wolf}}{{De~Wolf}}}
\bibcite{hitchcock1927expression}{{6}{1927}{{Hitchcock}}{{Hitchcock}}}
\newlabel{lem:tensor}{{1}{6}{}{lem.1}{}}
\newlabel{lem:bracketing}{{2}{6}{Bracketing number for bounded low rank tensor}{lem.2}{}}
\bibcite{9119759}{{7}{2020}{{{Ibrahim} et~al.}}{{{Ibrahim}, {Fu}, and {Li}}}}
\bibcite{kadmon2018statistical}{{8}{2018}{{Kadmon and Ganguli}}{{Kadmon and Ganguli}}}
\bibcite{montanari2018spectral}{{9}{2018}{{Montanari and Sun}}{{Montanari and Sun}}}
\bibcite{mu2014square}{{10}{2014}{{Mu et~al.}}{{Mu, Huang, Wright, and Goldfarb}}}
